<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Lab03 Ceph存储集群实践 #  实验目的 #    了解Ceph存储的基本工作原理
  建立对分布式存储的初步认识
  实验说明 #  本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。
当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建3台以上的机器进行实验（或者使用其他公有云产品中的机器）。
对于使用自己的机器的同学，建议采用CentOS系统（这样Kernel版本不至于太新，减少出错的可能），每台机器配置为2核2G即可，并且每台机器至少附带一个全新的大于5G的虚拟磁盘。
请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。
Ceph是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph也不例外。很多时候，需要对Linux Kernel和Linux网络管理有基本的了解。
实验中遇到的困难请及时在课程微信群中抛出。
实验文档不可能将整个Ceph的文档都搬过来，因此，在实验的整个过程中，请务必将 Ceph官方文档也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考 Ceph官方文档的相关讨论。
另外，因为Ceph是Red Hat主导开发的产品，因此，Red Hat也有关于Ceph完整的技术介绍， 它的文档同样值得参考，比如，你在这里可以找到它的 Ceph部署指南，甚至在很多方面比Ceph官方更加详实。
 在中文互联网上，你很容易找到 Ceph的中文文档。但它有很多过时的内容，比如它还在使用ceph-deploy这个官方已经不再维护的工具（这个工具同样不支持新版的Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。
除了中文文档，你还会找到很多良莠不齐的Blog，这些文章很多也是过时的，在使用时同样务必加以甄别。
在实验过程中，在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果。当然，勇敢试错是值得鼓励的，但请做好从头开始的准备。
 概述 #  Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？
试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。
最初，你将网站部署在一个装有500G硬盘的服务器上。随着时间的流逝，500G的硬盘逐渐被填满。现在你有两种选择。
  纵向拓展。在服务器上加装硬盘，甚至你可以使用LVM将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。
  横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。
  第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。
但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：
  如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？
  如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。
  随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。
  ……">
<meta name=theme-color content="#FFFFFF">
<meta name=color-scheme content="light dark"><meta property="og:title" content="Lab03 Ceph存储集群实践">
<meta property="og:description" content="Lab03 Ceph存储集群实践 #  实验目的 #    了解Ceph存储的基本工作原理
  建立对分布式存储的初步认识
  实验说明 #  本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。
当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建3台以上的机器进行实验（或者使用其他公有云产品中的机器）。
对于使用自己的机器的同学，建议采用CentOS系统（这样Kernel版本不至于太新，减少出错的可能），每台机器配置为2核2G即可，并且每台机器至少附带一个全新的大于5G的虚拟磁盘。
请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。
Ceph是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph也不例外。很多时候，需要对Linux Kernel和Linux网络管理有基本的了解。
实验中遇到的困难请及时在课程微信群中抛出。
实验文档不可能将整个Ceph的文档都搬过来，因此，在实验的整个过程中，请务必将 Ceph官方文档也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考 Ceph官方文档的相关讨论。
另外，因为Ceph是Red Hat主导开发的产品，因此，Red Hat也有关于Ceph完整的技术介绍， 它的文档同样值得参考，比如，你在这里可以找到它的 Ceph部署指南，甚至在很多方面比Ceph官方更加详实。
 在中文互联网上，你很容易找到 Ceph的中文文档。但它有很多过时的内容，比如它还在使用ceph-deploy这个官方已经不再维护的工具（这个工具同样不支持新版的Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。
除了中文文档，你还会找到很多良莠不齐的Blog，这些文章很多也是过时的，在使用时同样务必加以甄别。
在实验过程中，在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果。当然，勇敢试错是值得鼓励的，但请做好从头开始的准备。
 概述 #  Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？
试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。
最初，你将网站部署在一个装有500G硬盘的服务器上。随着时间的流逝，500G的硬盘逐渐被填满。现在你有两种选择。
  纵向拓展。在服务器上加装硬盘，甚至你可以使用LVM将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。
  横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。
  第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。
但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：
  如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？
  如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。
  随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。
  ……">
<meta property="og:type" content="article">
<meta property="og:url" content="https://bugitt.github.io/ns-labs/docs/table-of-contents/ceph/"><meta property="article:section" content="docs">
<meta property="article:modified_time" content="2021-11-30T19:47:24+08:00">
<title>Lab03 Ceph存储集群实践 | Network Storage Labs</title>
<link rel=manifest href=/ns-labs/manifest.json>
<link rel=icon href=/ns-labs/favicon.png type=image/x-icon>
<link rel=stylesheet href=/ns-labs/book.min.46181bc93375ba932026e753b37c40e6ff8bb16a9ef770c78bcc663e4577b1ba.css integrity="sha256-RhgbyTN1upMgJudTs3xA5v+LsWqe93DHi8xmPkV3sbo=" crossorigin=anonymous>
<script defer src=/ns-labs/flexsearch.min.js></script>
<script defer src=/ns-labs/en.search.min.828a0bde2ad812c7cb9f99c277605e9f9fa1d6f72321e9f8eca80d75543a343f.js integrity="sha256-gooL3irYEsfLn5nCd2Ben5+h1vcjIen47KgNdVQ6ND8=" crossorigin=anonymous></script>
<script defer src=/ns-labs/sw.min.e807e073195b27145d6580fa7d77604a8722176c4cd8c5b2c54bb1ca82ab66fa.js integrity="sha256-6AfgcxlbJxRdZYD6fXdgSociF2xM2MWyxUuxyoKrZvo=" crossorigin=anonymous></script>
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a class="flex align-center" href=/ns-labs/><span>Network Storage Labs</span>
</a>
</h2>
<div class=book-search>
<input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/>
<div class="book-search-spinner hidden"></div>
<ul id=book-search-results></ul>
</div>
<ul>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/>Table of Contents</a>
<ul>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/raid/>Lab01 RAID 阵列</a>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/virtual/>Lab02 虚拟化实验</a>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/ceph/ class=active>Lab03 Ceph存储集群实践</a>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/faq/>FAQ</a>
</li>
</ul>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/resources/>Resources</a>
<ul>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/resources/os/>操作系统</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href=https://github.com/bugitt/ns-labs target=_blank rel=noopener>
Github
</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/ns-labs/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>Lab03 Ceph存储集群实践</strong>
<label for=toc-control>
<img src=/ns-labs/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<nav id=TableOfContents>
<ul>
<li><a href=#lab03-ceph存储集群实践>Lab03 Ceph存储集群实践</a>
<ul>
<li><a href=#实验目的>实验目的</a></li>
<li><a href=#实验说明>实验说明</a></li>
<li><a href=#概述>概述</a></li>
<li><a href=#重要概念>重要概念</a>
<ul>
<li></li>
</ul>
</li>
<li><a href=#ceph部署>Ceph部署</a>
<ul>
<li><a href=#cephadm>Cephadm</a></li>
<li><a href=#部署前操作>部署前操作</a></li>
<li><a href=#部署>部署</a></li>
</ul>
</li>
<li><a href=#实验报告模板>实验报告模板</a></li>
</ul>
</li>
</ul>
</nav>
</aside>
</header>
<article class=markdown><h1 id=lab03-ceph存储集群实践>
Lab03 Ceph存储集群实践
<a class=anchor href=#lab03-ceph%e5%ad%98%e5%82%a8%e9%9b%86%e7%be%a4%e5%ae%9e%e8%b7%b5>#</a>
</h1>
<h2 id=实验目的>
实验目的
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e7%9b%ae%e7%9a%84>#</a>
</h2>
<ol>
<li>
<p>了解Ceph存储的基本工作原理</p>
</li>
<li>
<p>建立对分布式存储的初步认识</p>
</li>
</ol>
<h2 id=实验说明>
实验说明
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e8%af%b4%e6%98%8e>#</a>
</h2>
<p>本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。</p>
<p>当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建3台以上的机器进行实验（或者使用其他公有云产品中的机器）。</p>
<p>对于使用自己的机器的同学，建议采用CentOS系统（这样Kernel版本不至于太新，减少出错的可能），每台机器配置为2核2G即可，并且每台机器至少附带一个全新的大于5G的虚拟磁盘。</p>
<p>请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。</p>
<blockquote class="book-hint info">
<p>Ceph是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph也不例外。很多时候，需要对Linux Kernel和Linux网络管理有基本的了解。</p>
<p><strong>实验中遇到的困难请及时在课程微信群中抛出。</strong></p>
<p>实验文档不可能将整个Ceph的文档都搬过来，因此，在实验的整个过程中，请务必将
<a href=https://docs.ceph.com/en/pacific/>Ceph官方文档</a>也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考
<a href=https://docs.ceph.com/en/pacific/>Ceph官方文档</a>的相关讨论。</p>
<p>另外，因为Ceph是Red Hat主导开发的产品，因此，Red Hat也有关于Ceph完整的技术介绍，
<a href=https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5>它的文档</a>同样值得参考，比如，你在这里可以找到它的
<a href=https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/installation_guide/index>Ceph部署指南</a>，甚至在很多方面比Ceph官方更加详实。</p>
</blockquote>
<blockquote class="book-hint danger">
<p>在中文互联网上，你很容易找到
<a href=http://docs.ceph.org.cn/start/intro/>Ceph的中文文档</a>。但它有很多<strong>过时的内容</strong>，比如它还在使用<code>ceph-deploy</code>这个官方已经不再维护的工具（这个工具同样不支持新版的Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。</p>
<p>除了中文文档，你还会找到很多良莠不齐的Blog，这些文章很多也是过时的，在使用时同样务必加以甄别。</p>
<p>在实验过程中，<strong>在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果</strong>。当然，勇敢试错是值得鼓励的，但请做好从头开始的准备。</p>
</blockquote>
<h2 id=概述>
概述
<a class=anchor href=#%e6%a6%82%e8%bf%b0>#</a>
</h2>
<p>Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？</p>
<p>试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。</p>
<p>最初，你将网站部署在一个装有500G硬盘的服务器上。随着时间的流逝，500G的硬盘逐渐被填满。现在你有两种选择。</p>
<ol>
<li>
<p>纵向拓展。在服务器上加装硬盘，甚至你可以使用LVM将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。</p>
</li>
<li>
<p>横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。</p>
</li>
</ol>
<p>第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。</p>
<p>但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：</p>
<ul>
<li>
<p>如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？</p>
</li>
<li>
<p>如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。</p>
</li>
<li>
<p>随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。</p>
</li>
<li>
<p>……</p>
</li>
</ul>
<p>上面这些问题，正是Ceph这类分布式存储系统所要解决的问题。简单来说，Ceph是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供<strong>分布式</strong>、<strong>横向拓展</strong>、<strong>高度可靠性</strong>的存储系统。</p>
<blockquote class="book-hint info">
<p>对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下
<a href=https://pdos.csail.mit.edu/6.824/>MIT 6.824</a>的课程，并尽量完成它的全部实验。</p>
<p>在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B站上有
<a href=https://www.bilibili.com/video/BV1R7411t71W>翻译好的熟肉</a>。</p>
</blockquote>
<p>除此之外，Ceph的独特之处还在于，它在一个存储系统上，对外提供了三种类型的访问接口：</p>
<ul>
<li>
<p>文件存储。简单来说，你可以将Ceph的存储池抽象为一个文件系统，并挂载到某个目录上，然后像读写本地文件一样，在这个新的目录上创建、读写、删除文件。并且该文件系统可以同时被多台机器同时挂载，并被同时读写。从而实现多台机器间的存储共享。</p>
</li>
<li>
<p>对象存储。Ceph提供了对象存储网关，并同时提供了S3和Swift风格的API接口。你可以使用这些接口上传和下载文件。</p>
</li>
<li>
<p>块存储。Ceph还能提供块存储的抽象。即客户端（集群外的机器）通过块存储接口访问的“所有数据按照固定的大小分块，每一块赋予一个用于寻址的编号。”客户端可以像使用硬盘这种块设备一样，使用这些块存储的接口进行数据的读写。（一般这种块设备的读写都是由操作系统代劳的。操作系统会对块设备进行分区等操作，并在其上部署文件系统，应用程序和用户看到直接看到的是文件系统的接口（也就是文件存储））。</p>
</li>
</ul>
<p>
<img src=https://cdn.loheagn.com/123326.jpg alt></p>
<p>需要注意的是，虽然Ceph对外提供了上面这三种不同类型的存储接口，但其底层会使用相同的逻辑对接收的数据进行分块和存储。</p>
<h2 id=重要概念>
重要概念
<a class=anchor href=#%e9%87%8d%e8%a6%81%e6%a6%82%e5%bf%b5>#</a>
</h2>
<p>一个Ceph集群必须包含三种类型的进程：Monitor、OSD和Manager。其中，Monitor和OSD是最核心的两类进程。</p>
<h4 id=monitor-和-osd>
Monitor 和 OSD
<a class=anchor href=#monitor-%e5%92%8c-osd>#</a>
</h4>
<p>Monitor进程负责维护整个系统的状态信息，这些状态信息包括当前的Ceph集群的拓扑结构等，这些信息对Ceph集群中各个进程的通信来说非常关键。除此之外，Monitor进程还负责充当外界（也就是官方文档中总是提到的“Client”）与OSD进程交流的媒介。</p>
<p>OSD进程则负责进行真正的数据存储。如下图所示，外界传送给Ceph集群的数据（不管是通过文件存储、对象存储还是块存储的接口）都将被转化为一个个对象（object）。这些object将经由OSD进程存储到磁盘中。</p>
<p>
<img src=https://cdn.loheagn.com/134513.jpg alt></p>
<p>简单来说，当一个Client试图向Ceph集群读写数据时，将发生以下步骤：</p>
<ol>
<li>
<p>Client向Monitor进程请求一个token校验信息</p>
</li>
<li>
<p>Monitor生成token校验信息，并将其返回给Client</p>
</li>
<li>
<p>Monitor同时会将token校验信息同步OSD进程</p>
</li>
<li>
<p>Client携带着Monitor返回的token校验信息向对应的token发送数据读写请求</p>
</li>
<li>
<p>OSD进程将数据存储到合适的位置，或从合适的位置读出数据</p>
</li>
<li>
<p>OSD进程向Client返回数据</p>
</li>
</ol>
<blockquote class="book-hint info">
<p>以上读写数据的流程是经过极致简化的，主要是为了帮助大家建立对Monitor进程和OSD进程所起的作用的感性认识。</p>
<p>想要了解详情，请阅读
<a href=https://docs.ceph.com/en/pacific/architecture/>Ceph的文档 - Architecture</a>。</p>
</blockquote>
<h4 id=manager>
Manager
<a class=anchor href=#manager>#</a>
</h4>
<p>Manager进程主要负责跟踪当前集群的运行时状况，包括当前集群的存储利用率、存储性能等等。同时，它还负责提供Ceph Dashboard、RESTful接口的外部服务。</p>
<blockquote class="book-hint info">
<p>需要注意的是，Ceph集群中有<strong>三类</strong>这样的进程，但不是每个进程只有<strong>一个</strong>。</p>
<p>我们之前提到过，Ceph是一个有很高容错性的分布式系统，而达到高容错性的一个很重要的方式就是“<strong>冗余</strong>”。</p>
<p>比如，对于Monitor进程来讲，集群中仅有一个就够用了。但如果运行这一个Monitor进程的机器挂了，那么整个集群就会瘫痪（Client将不知道该跟谁通信来拿到校验信息和集群状态信息等）。因此，一个高可用的Ceph集群中会包含多个执行几乎相同任务的运行在不同机器上的Monitor进程；这样挂了一个，Client还可以跟剩下的通信，整个集群依旧可以正常对外提供服务。同样的道理，OSD进程和Manager进程也有多个副本。</p>
<p>另外，Ceph为了保证数据的可靠性（也就是说Client存储进来的数据不能丢失）——注意区分其与整个系统可靠性的区别——在默认情况下，会将每份数据存储<strong>3份</strong>，每份都会存储在不同的OSD上（鸡蛋不能放到同一个篮子里）。这样，即使有部分OSD挂掉，也能保证大部分数据不会丢失。因此，一个健康的Ceph集群要求至少同时存在三个健康的OSD进程（当然，这个默认的数值可以更改）。</p>
</blockquote>
<h4 id=pool-与-placement-grouppg>
Pool 与 Placement Group（PG）
<a class=anchor href=#pool-%e4%b8%8e-placement-grouppg>#</a>
</h4>
<p>请查阅Ceph的相关文档，阐述 Pool、Placement Group 与 OSD 之间的关系。</p>
<h2 id=ceph部署>
Ceph部署
<a class=anchor href=#ceph%e9%83%a8%e7%bd%b2>#</a>
</h2>
<p>本节内容的目标是创建一个可用的Ceph集群。其中包括，至少一个Monitor进程、至少一个Manager进程、至少三个OSD进程。</p>
<blockquote class="book-hint info">
<p>使用云平台提供的机器进行实验的同学，请直接使用root用户登录，密码是<code>&shieshuyuan21</code>。</p>
<p>登录后，请首先使用<code>passwd</code>命令修改密码。</p>
</blockquote>
<p>Ceph官方提供了
<a href=https://docs.ceph.com/en/pacific/install/>多种部署方式</a>。</p>
<p>在本实验文档中，我们采用
<a href=https://docs.ceph.com/en/pacific/cephadm/#cephadm>Cephadm</a>作为部署工具。Cephadm也是官方推荐的部署和管理Ceph集群的工具，它不仅可以用来部署Ceph，还可以在安装完成后，用来管理集群（添加和移除节点、开启rgw等）。</p>
<p>当然，如果你对其他部署方式感兴趣的话，也可以尝试；比如，对Kubernetes比较感兴趣的同学，可以尝试使用
<a href=https://rook.io/>Rook</a>进行部署。</p>
<h3 id=cephadm>
Cephadm
<a class=anchor href=#cephadm>#</a>
</h3>
<p>Cephadm是基于“容器技术（Container）”进行工作的，每个Ceph的工作进程都运行在相互隔离的容器中。Cephadm支持使用Docker和Podman作为容器运行时。在部署时，Cephadm将首先检查本机中安装的容器运行时类型，当Docker与Podman并存时，将首先使用Podman（毕竟Podman也是Red Hat的产品）。</p>
<p>在部署时，Cephadm会首先在本地启动一个mini的Ceph集群，其中包括Ceph集群最基本的Monitor进程和Manager进程（当然，这两个进程都是通过容器形式运行起来的）。这个mini集群在某种程度上来说也是合法的，只不过其基本不能对外提供任何功能。随后，我们将继续使用Cephadm提供的工具，将其他机器（后文中也会称之为“节点”）加入到集群中（也就是在其他节点中启动Ceph的Monitor、Manager、OSD等进程），从而构建一个完整可用的集群。</p>
<p>下面是Red Hat给出使用Cephadm构建的集群的架构图。</p>
<p>
<img src=https://cdn.loheagn.com/090719.jpg alt></p>
<p>图中的“Container”指的就是我们上面所说的“容器”。</p>
<p>图中最左边的Bootstrap Host就是我们执行Cephadm相关命令的机器（事实上，在整个集群的构建过程中，除了修改IP和联网等操作外，我们都只会在这个Bootstrap Host上进行操作）。我们在Bootstrap Host执行的针对其他节点的操作，都是Cephadm通过ssh的方式发送给对应节点的。</p>
<p>这个图目前大家心里有个印象就好，在后续的实验操作中，可以反复回来对照检查。</p>
<h3 id=部署前操作>
部署前操作
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e5%89%8d%e6%93%8d%e4%bd%9c>#</a>
</h3>
<h4 id=联网>
联网
<a class=anchor href=#%e8%81%94%e7%bd%91>#</a>
</h4>
<p>部署过程中的操作尽量都联网进行。云平台的Linux机器联网操作请参考：</p>
<blockquote>
<p>推荐使用
<a href=https://github.com/Dr-Bluemond/srun>Dr-Bluemond/srun</a>提供的工具。云平台提供已经编译好的Linux64位版本。可以这样获取：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>wget https://scs.buaa.edu.cn/scsos/tools/linux/buaalogin
chmod +x ./buaalogin
</code></pre></div><p>使用前请使用<code>config</code>命令配置一下校园网用户名和密码（注意，如果用户名中有英文的话，请大小写都尝试一下）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./buaalogin config
</code></pre></div><p>配置完成后，使用<code>login</code>命令登录即可：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./buaalogin login
</code></pre></div><p>或，直接：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./buaalogin
</code></pre></div><p>如果想作为系统命令使用的话（注意替换合适的安装路径）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo install ./buaalogin /usr/local/bin
</code></pre></div></blockquote>
<h4 id=安装cephadm>
安装Cephadm
<a class=anchor href=#%e5%ae%89%e8%a3%85cephadm>#</a>
</h4>
<blockquote class="book-hint info">
对于云平台提供的虚拟机，Cephadm相关的工具已经安装好，本步骤可以略过。
</blockquote>
<blockquote class="book-hint info">
本步骤需要在各台机器上都要完成
</blockquote>
<p>Cephadm也是个命令行工具，在使用前也需要额外安装这个工具。</p>
<ol>
<li>
<p>保证系统已经安装好curl、Python3、Podman/Docker(推荐使用Podman，都是自家RedHat的产品)</p>
</li>
<li>
<p>下载Cephadm：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl --silent --remote-name --location https://scs.buaa.edu.cn/scsos/ceph/cephadm
</code></pre></div><p>赋予可执行权限；</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>chmod +x ./cephadm
</code></pre></div></li>
<li>
<p>安装相关工具（请务必保持联网状态，可能会耗费比较久的时间，请耐心等待）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./cephadm add-repo --release octopus
./cephadm install
cephadm install ceph-common
</code></pre></div></li>
</ol>
<h4 id=配置静态ip修改hostname>
配置静态IP，修改Hostname
<a class=anchor href=#%e9%85%8d%e7%bd%ae%e9%9d%99%e6%80%81ip%e4%bf%ae%e6%94%b9hostname>#</a>
</h4>
<blockquote class="book-hint info">
本步骤需要在各台机器上都要完成
</blockquote>
<p>Ceph集群的各个机器之间是通过IP地址进行通信的，而云平台的机器在重启后通过DHCP获取的IP可能会发生变化，这将给实验带来一定的干扰。</p>
<p>因此，建议在正式开始实验前，为机器配置静态IP（即，将你此时的虚拟机的IP固定下来，即禁用DHCP，改为手动指定IP、网关、DNS服务器等）。对于使用个人机器进行实验的同学，同样也建议配置一下静态IP。</p>
<blockquote class="book-hint info">
使用云平台提供的机器的同学，请参考下方给出的CentOS机器的配置方法。
</blockquote>
<div class=book-tabs><input type=radio class=toggle name="tabs-Static IP" id="tabs-Static IP-0" checked>
<label for="tabs-Static IP-0">CentOS</label>
<div class="book-tabs-content markdown-inner"><p>对于CentOS的机器，配置IP可以手动编辑<code>/etc/sysconfig/network-scripts/</code>下的配置文件。</p>
<p>也可以采用NetworkManager提供的下面这种图形化的终端配置方法。</p>
<p>登录到虚拟机后，使用<code>nmtui</code>编辑需要配置静态IP的网卡：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nmtui edit ens150
</code></pre></div><blockquote class="book-hint danger">
<h1 id=指定正确的网卡>
指定正确的网卡
<a class=anchor href=#%e6%8c%87%e5%ae%9a%e6%ad%a3%e7%a1%ae%e7%9a%84%e7%bd%91%e5%8d%a1>#</a>
</h1>
<p>上面这条命令中的<code>ens150</code>指的是你要指定静态IP的网卡名称。请根据自己的实际情况指定网卡名称。</p>
<p>如何查看自己的网卡名称？</p>
<p>在终端中输入<code>ip a</code>，将得到大致如下的输出：</p>
<p>
<img src=https://cdn.loheagn.com/150722.png alt></p>
<p>可以看到，这里列出了当前机器上的所有网卡及其上的IP等信息，并且每个都有编号。</p>
<p>找到那个标有你的IP地址的网卡信息，即可找到它的网卡网卡名称。</p>
<p>比如，在上图中，你要固定的IP是<code>10.251.253.10</code>，找到它对应的那一组网卡信息，即可看到网卡名称是<code>ens160</code>。</p>
</blockquote>
<p>在打开的如下图所示的界面中，编辑<code>IPv4 CONFIGURATION</code>下面的几项。</p>
<p>
<img src=https://cdn.loheagn.com/064525.png alt></p>
<p>首先，将<code>Automatic</code>切换为<code>Manual</code>。意思是，不希望通过DHCP自动获取IP，而是手动指定IP。</p>
<p>然后，编辑<code>Address</code>，填入你当前的IP地址，并给出子网掩码的位数。对于云平台的机器子网掩码位数是<code>22</code>；使用本地机器的同学，请根据实际情况配置。</p>
<p>然后，编辑<code>Gateway</code>，即网关地址。对于云平台的机器，这一地址是<code>10.251.252.1</code>；使用本地机器的同学，请根据实际情况配置。</p>
<p>然后，编辑<code>DNS servers</code>，即DNS服务器地址。对于校园网内的机器，都可以将DNS地址指定为<code>202.112.128.50</code>。</p>
<p>之后，选择最下面的<code>OK</code>退出即可。</p>
<p>总而言之，如果使用云平台的机器，最终配置的结果与下图所示应该一直（仅IP地址不同）。</p>
<p>
<img src=https://cdn.loheagn.com/150020.png alt></p>
</div><input type=radio class=toggle name="tabs-Static IP" id="tabs-Static IP-1">
<label for="tabs-Static IP-1">Debian 及 Ubuntu 16.04</label>
<div class="book-tabs-content markdown-inner"><p>直接编辑 <code>/etc/network/interfaces</code> 文件即可。具体可以参考
<a href=https://michael.mckinnon.id.au/2016/05/05/configuring-ubuntu-16-04-static-ip-address/>这篇文章</a>。</p>
<p>其中，对于云平台的机器，需要指定<code>gateway</code>为<code>10.251.253.10</code>，<code>netmask</code>为<code>255.255.252.0</code>，<code>dns-nameservers</code>为<code>202.112.128.50</code>。</p>
</div><input type=radio class=toggle name="tabs-Static IP" id="tabs-Static IP-2">
<label for="tabs-Static IP-2">Ubuntu 18.04 及以上</label>
<div class="book-tabs-content markdown-inner"><p>Ubuntu 18.04开始，Ubuntu开始使用Netplan来管理网络连接。因此，配置静态IP需要编辑 <code>/etc/netplan/</code> 下的配置文件。具体可以参考
<a href=https://linuxize.com/post/how-to-configure-static-ip-address-on-ubuntu-18-04/>这篇文章</a>。</p>
<p>其中，对于云平台的机器，需要指定<code>gateway</code>为<code>10.251.253.10</code>，<code>netmask</code>为<code>255.255.252.0</code>，<code>nameservers</code>为<code>202.112.128.50</code>。</p>
</div></div>
<blockquote class="book-hint danger">
<p>对于使用云平台的机器的同学，配置静态IP时，请<strong>务必务必</strong>保证使用现有的IP。</p>
<p>也就是说，配置静态IP前后，你的机器的IP应该始终相同。</p>
</blockquote>
<p>配置完静态IP后，还需要修改一下机器的hostname，以区分集群内的各台机器。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>echo <span style=color:#e6db74>&#39;你的机器的Hostname&#39;</span> &gt; /etc/hostname
</code></pre></div><p>机器的hostname其实可以任意指定，但最好有意义，并携带上你的学号，以防止云平台上几十台实验虚拟机的hostname发生冲突。</p>
<p>比如，如果你的学号是<code>15213304</code>，可以将第一台机器的hostname指定为<code>ceph-01-15213304</code>，第二台机器的hostname指定为<code>ceph-02-15213304</code>。</p>
<p>配置完静态IP和hostname后，请使用 <code>reboot</code> 命令重启机器。</p>
<h3 id=部署>
部署
<a class=anchor href=#%e9%83%a8%e7%bd%b2>#</a>
</h3>
<blockquote class="book-hint info">
基于Cephadm的便捷性，部署部分的操作只需要在<strong>选定的一台</strong>机器（我们把这台机器称为Bootstrap Host上执行即可。
</blockquote>
<blockquote class="book-hint info">
在进行部署操作前，请务必保证所有机器处于联网状态。
</blockquote>
<h4 id=bootstrap>
BootStrap
<a class=anchor href=#bootstrap>#</a>
</h4>
<p>我们首先需要在一台选定的机器上，使用<code>cephadm</code>启动一个mini集群。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cephadm bootstrap --image harbor.scs.buaa.edu.cn/ceph/ceph:v16 --mon-ip *&lt;mon-ip&gt;*
</code></pre></div><p>请将<code>*&lt;mon-ip>*</code>替换为你执行这命令的机器的IP。</p>
<p>如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cephadm --image harbor.scs.buaa.edu.cn/ceph/ceph:v16 bootstrap --mon-ip 10.251.219.248
</code></pre></div><p>上面这条命令中，<code>--image</code>制定了Cephadm启动容器时使用的镜像名称，<code>--mon-ip</code>指定了Cephadm要在哪个机器上启动一个mini集群。</p>
<p>更详细地，这条命令将会做如下事情：</p>
<blockquote>
<ul>
<li>
<p>Create a monitor and manager daemon for the new cluster on the local host.</p>
</li>
<li>
<p>Generate a new SSH key for the Ceph cluster and add it to the root user&rsquo;s /root/.ssh/authorized_keys file.</p>
</li>
<li>
<p>Write a copy of the public key to /etc/ceph/ceph.pub.</p>
</li>
<li>
<p>Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster.</p>
</li>
<li>
<p>Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring.</p>
</li>
<li>
<p>Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring.</p>
</li>
</ul>
</blockquote>
<p>命令执行完成后，我们可以通过<code>ceph -s</code>查看当前集群的状态。</p>
<p>
<img src=https://cdn.loheagn.com/074508.png alt></p>
<p>可以看到，确实启动了一个Monitor进程和一个Manager进程。</p>
<p>另外，我们还注意到，当前集群的健康状态是<code>HEALTH_WARN</code>，原因下面也列出来了：<code>OSD count 0 &lt; osd_pool_default_size 3</code>。这是因为当前Ceph集群默认的每个Pool的副本数应该是3（即，Ceph中存储的每份数据必须复制3份，放在3个不同的OSD中），但我们OSD的进程数是0。不用着急，马上我们就会创建足够的OSD进程。</p>
<p>
<img src=https://cdn.loheagn.com/074750.png alt></p>
<p>mini集群启动完成后，可以使用<code>podman ps</code>查看当前启动的容器的状态：</p>
<p>
<img src=https://cdn.loheagn.com/091703.png alt></p>
<p>可以看到，最上面那两个容器，对应的就是ceph集群的Monitor进程和Manager进程。</p>
<h4 id=ceph-dashboard可选>
Ceph Dashboard（可选）
<a class=anchor href=#ceph-dashboard%e5%8f%af%e9%80%89>#</a>
</h4>
<p>注意看<code>bootstrap</code>指令的输出，你可以看到一段这样的内容：</p>
<p>
<img src=https://cdn.loheagn.com/074911.png alt></p>
<p>显然，这是在告诉我们，cephadm同样启动了一个<code>Ceph Dashboard</code>，这是一个Ceph的管理前端。通过访问这个页面，我们就可以以可视化的方式观察到当前集群的状态。</p>
<p>这段信息中的<code>ceph-01</code>代指的就是你当前这台机器的IP（当然，如果你的Hostname不是<code>ceph-01</code>的话，得到的将是其他的结果）。用你的机器的IP替换这个<code>ceph-01</code>，尝试在浏览器访问它。</p>
<p>是不是访问失败了？这是因为Ceph Dashboard默认使用的是HTTPS协议，而且它用的HTTPS证书还是自己签发的。。。为了能正常访问，我们可以手动禁用SSL：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph config set mgr mgr/dashboard/ssl false
</code></pre></div><p>但禁用SSL后，Dashboard服务将默认监听8080端口。但在CentOS中，8080端口默认是被防火墙屏蔽的。</p>
<p>你可以选择手动打开防火墙的8080端口；也可以像下面这样，将Dashboard服务的监听端口手动改为8443（因为这个端口就是使用HTTPS时Dashboard的监听端口，在刚才的Bootstrap时已经在防火墙中打开了）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph config set mgr mgr/dashboard/server_port <span style=color:#ae81ff>8443</span>
</code></pre></div><p>然后，重启该Dashboard服务，使刚才的配置生效。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph mgr module disable dashboard
ceph mgr module enable dashboard
</code></pre></div><p>然后，查看当前的服务状态（如果输出为空的话，耐心多等一会儿）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph mgr services
</code></pre></div><p>
<img src=https://cdn.loheagn.com/080831.png alt></p>
<p>现在，你应该可以正常访问Dashboard服务了。注意，用户名和密码是我们前面提到的Bootstrap命令输出的那堆信息中提到的。</p>
<p>
<img src=https://cdn.loheagn.com/081143.png alt></p>
<h4 id=添加其他节点>
添加其他节点
<a class=anchor href=#%e6%b7%bb%e5%8a%a0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9>#</a>
</h4>
<blockquote class="book-hint danger">
添加其他节点前，请保证所有节点都处于联网状态
</blockquote>
<p>接下来，我们将把其他机器添加到现有的这个mini集群中来。</p>
<p>前面提到过，cephadm是通过ssh协议与其他机器通信的。所以，这里需要首先把Bootstrap Host机器的公钥copy到其他的所有机器：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh-copy-id -f -i /etc/ceph/ceph.pub root@*&lt;new-host&gt;*
</code></pre></div><p>例如，如果你的有一台机器的IP是<code>10.252.252.40</code>，那么这条命令应该是：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh-copy-id -f -i /etc/ceph/ceph.pub root@10.252.252.40
</code></pre></div><p>处理完所有的机器后，就可以正式将它们加入到mini集群中来了：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch host add *&lt;newhost&gt;* <span style=color:#f92672>[</span>*&lt;ip&gt;*<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>*&lt;label1&gt; ...*<span style=color:#f92672>]</span>
</code></pre></div><p>例如，你有一台机器的Hostname是<code>ceph-02</code>，IP是<code>10.252.252.40</code>，那么这条命令应该是：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch host add ceph-02 10.252.252.40
</code></pre></div><p>添加完成后，你可以通过<code>ceph -s</code>查看当前集群状态的变化。也可以通过Ceph Dashboard看到变化。</p>
<h4 id=创建osd进程>
创建OSD进程
<a class=anchor href=#%e5%88%9b%e5%bb%baosd%e8%bf%9b%e7%a8%8b>#</a>
</h4>
<p>我们知道，OSD进程是真正用来做数据读写的进程。我们可以用一块专门的磁盘交给OSD进程来读写数据，ceph集群所存储的数据就将保存在这些磁盘中。</p>
<p>这些被用来交给OSD进程管理的磁盘，应该满足以下条件：</p>
<blockquote>
<ul>
<li>
<p>The device must have no partitions.</p>
</li>
<li>
<p>The device must not have any LVM state.</p>
</li>
<li>
<p>The device must not be mounted.</p>
</li>
<li>
<p>The device must not contain a file system.</p>
</li>
<li>
<p>The device must not contain a Ceph BlueStore OSD.</p>
</li>
<li>
<p>The device must be larger than 5 GB.</p>
</li>
</ul>
</blockquote>
<p>简单来说，就是将一块干净的磁盘插入机器后，什么都不用做就好。</p>
<p>在云平台分配的虚拟机中，每台机器都额外插入了一块这样干净的磁盘。可以通过<code>fdisk -l</code>来查看：</p>
<p>
<img src=https://cdn.loheagn.com/090352.png alt></p>
<p>注意看上面这两块磁盘：</p>
<ul>
<li>
<p>第一个名称是<code>/dev/sda</code>，容量是16G，有两个分区：<code>/dev/sda1</code>，<code>/dev/sda2</code>。这就是我们现在在使用的这个系统所用的磁盘，系统数据都存储在这个磁盘中。</p>
</li>
<li>
<p>第二个名称是<code>/dev/sdb</code>，容量是10G，没有任何分区。这就是我们即将交给OSD管理的磁盘。</p>
</li>
</ul>
<p>使用下面的命令来创建OSD进程：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch daemon add osd *&lt;hostname&gt;*:*&lt;device-name&gt;*
</code></pre></div><p>比如，你要在主机<code>ceph-01</code>的名称为<code>/dev/sdb</code>的磁盘上创建OSD进程，那么命令应该是：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch daemon add osd ceph-01:/dev/sdb
</code></pre></div><blockquote class="book-hint info">
<p>这条命令默认不会有输出创建OSD进程的详细信息，也就是说，如果该命令很耗时的话，那么你将在什么输出都没有的情况下等待较长时间，这可能令人发慌。你可以加上<code>--verbose</code>参数（缩写为<code>-v</code>），来让它输出详细信息。</p>
<p>比如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch daemon add osd ceph-01:/dev/sdb --verbose
</code></pre></div>
</blockquote>
<p>执行完成后，可以使用<code>ceph -s</code>查看当前的集群状态，可以发现已经有一个OSD进程加入了进来。</p>
<p>
<img src=https://cdn.loheagn.com/105544.png alt></p>
<p>使用同样的方法，将所有节点的附加硬盘都加入进来。</p>
<h2 id=实验报告模板>
实验报告模板
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e6%8a%a5%e5%91%8a%e6%a8%a1%e6%9d%bf>#</a>
</h2>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown># Lab03 Ceph存储实践
<span style=color:#66d9ef>
</span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>班级：
</span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>学号：
</span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>姓名：
</span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>合作者们：[&#39;学号-姓名&#39;, &#39;学号-姓名&#39;]
</span><span style=font-style:italic></span>
---

<span style=color:#75715e>## 实验内容
</span><span style=color:#75715e></span>
<span style=color:#75715e>### 基本概念思考
</span><span style=color:#75715e></span>
回答下列问题：

<span style=color:#75715e>#### Pool、Placement Group 与 OSD 之间的关系
</span><span style=color:#75715e></span>
<span style=color:#75715e>### Ceph部署
</span><span style=color:#75715e></span>
<span style=color:#75715e>&lt;!-- 本部分内容可以根据部署方式的不同进行不同的改动，这里是使用Cephadm部署的模板 --&gt;</span>

<span style=color:#75715e>#### 实验前置准备
</span><span style=color:#75715e></span>
<span style=color:#75715e>&lt;!-- 可写内容包括： --&gt;</span>
<span style=color:#75715e>&lt;!-- 云平台/本机，操作系统的具体版本，各台机器的Hostname --&gt;</span>
<span style=color:#75715e>&lt;!-- 一些前置工具的安装、配静态IP、改Hostname的记录 --&gt;</span>

<span style=color:#75715e>#### Bootstrap
</span><span style=color:#75715e></span>
<span style=color:#75715e>#### 添加 Host
</span><span style=color:#75715e></span>
<span style=color:#75715e>#### 创建 OSD 
</span><span style=color:#75715e></span>
<span style=color:#75715e>## 实验总结与心得
</span><span style=color:#75715e></span>
</code></pre></div></article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
<div><a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/commit/236bc6a130bbfab685d94bca53aced35e10fc163 title="Last modified by Yinghao Zhu | November 30, 2021" target=_blank rel=noopener>
<img src=/ns-labs/svg/calendar.svg class=book-icon alt=Calendar>
<span>November 30, 2021</span>
</a>
</div>
<div>
<a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/edit/main/exampleSite/content/docs/table-of-contents/ceph.md target=_blank rel=noopener>
<img src=/ns-labs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span>
</a>
</div>
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
</footer>
<div class=book-comments>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<nav id=TableOfContents>
<ul>
<li><a href=#lab03-ceph存储集群实践>Lab03 Ceph存储集群实践</a>
<ul>
<li><a href=#实验目的>实验目的</a></li>
<li><a href=#实验说明>实验说明</a></li>
<li><a href=#概述>概述</a></li>
<li><a href=#重要概念>重要概念</a>
<ul>
<li></li>
</ul>
</li>
<li><a href=#ceph部署>Ceph部署</a>
<ul>
<li><a href=#cephadm>Cephadm</a></li>
<li><a href=#部署前操作>部署前操作</a></li>
<li><a href=#部署>部署</a></li>
</ul>
</li>
<li><a href=#实验报告模板>实验报告模板</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</aside>
</main>
</body>
</html>