<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Lab03 Ceph 存储集群实践 #  实验目的 #    了解 Ceph 存储的基本工作原理
  建立对分布式存储的初步认识
  实验说明 #  本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。
当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建 3 台以上的机器进行实验（或者使用其他公有云产品中的机器）。
对于使用自己的机器的同学，建议采用 CentOS 系统（这样 Kernel 版本不至于太新，减少出错的可能），每台机器配置为 2 核 2G 即可，并且每台机器至少附带一个全新的大于 5G 的虚拟磁盘。
请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。
Ceph 是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph 也不例外。很多时候，需要对 Linux Kernel 和 Linux 网络管理有基本的了解。
实验中遇到的困难请及时在课程微信群中抛出。
实验文档不可能将整个 Ceph 的文档都搬过来，因此，在实验的整个过程中，请务必将 Ceph 官方文档也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考 Ceph 官方文档的相关讨论。
另外，因为 Ceph 是 Red Hat 主导开发的产品，因此，Red Hat 也有关于 Ceph 完整的技术介绍， 它的文档同样值得参考，比如，你在这里可以找到它的 Ceph 部署指南，甚至在很多方面比 Ceph 官方更加详实。
 在中文互联网上，你很容易找到 Ceph 的中文文档。但它有很多过时的内容，比如它还在使用ceph-deploy这个官方已经不再维护的工具（这个工具同样不支持新版的 Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。">
<meta name=theme-color content="#FFFFFF">
<meta name=color-scheme content="light dark"><meta property="og:title" content="Lab03 Ceph存储集群实践">
<meta property="og:description" content="Lab03 Ceph 存储集群实践 #  实验目的 #    了解 Ceph 存储的基本工作原理
  建立对分布式存储的初步认识
  实验说明 #  本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。
当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建 3 台以上的机器进行实验（或者使用其他公有云产品中的机器）。
对于使用自己的机器的同学，建议采用 CentOS 系统（这样 Kernel 版本不至于太新，减少出错的可能），每台机器配置为 2 核 2G 即可，并且每台机器至少附带一个全新的大于 5G 的虚拟磁盘。
请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。
Ceph 是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph 也不例外。很多时候，需要对 Linux Kernel 和 Linux 网络管理有基本的了解。
实验中遇到的困难请及时在课程微信群中抛出。
实验文档不可能将整个 Ceph 的文档都搬过来，因此，在实验的整个过程中，请务必将 Ceph 官方文档也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考 Ceph 官方文档的相关讨论。
另外，因为 Ceph 是 Red Hat 主导开发的产品，因此，Red Hat 也有关于 Ceph 完整的技术介绍， 它的文档同样值得参考，比如，你在这里可以找到它的 Ceph 部署指南，甚至在很多方面比 Ceph 官方更加详实。
 在中文互联网上，你很容易找到 Ceph 的中文文档。但它有很多过时的内容，比如它还在使用ceph-deploy这个官方已经不再维护的工具（这个工具同样不支持新版的 Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://bugitt.github.io/ns-labs/docs/table-of-contents/ceph/"><meta property="article:section" content="docs">
<meta property="article:modified_time" content="2021-12-02T11:16:15+08:00">
<title>Lab03 Ceph存储集群实践 | Network Storage Labs</title>
<link rel=manifest href=/ns-labs/manifest.json>
<link rel=icon href=/ns-labs/favicon.png type=image/x-icon>
<link rel=stylesheet href=/ns-labs/book.min.46181bc93375ba932026e753b37c40e6ff8bb16a9ef770c78bcc663e4577b1ba.css integrity="sha256-RhgbyTN1upMgJudTs3xA5v+LsWqe93DHi8xmPkV3sbo=" crossorigin=anonymous>
<script defer src=/ns-labs/flexsearch.min.js></script>
<script defer src=/ns-labs/en.search.min.dbc3c1a6100efb342db1cd132dae95777b636b3dcfd19ce72d21e0ba5d2a535e.js integrity="sha256-28PBphAO+zQtsc0TLa6Vd3tjaz3P0ZznLSHgul0qU14=" crossorigin=anonymous></script>
<script defer src=/ns-labs/sw.min.e807e073195b27145d6580fa7d77604a8722176c4cd8c5b2c54bb1ca82ab66fa.js integrity="sha256-6AfgcxlbJxRdZYD6fXdgSociF2xM2MWyxUuxyoKrZvo=" crossorigin=anonymous></script>
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a class="flex align-center" href=/ns-labs/><span>Network Storage Labs</span>
</a>
</h2>
<div class=book-search>
<input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/>
<div class="book-search-spinner hidden"></div>
<ul id=book-search-results></ul>
</div>
<ul>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/>Table of Contents</a>
<ul>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/raid/>Lab01 RAID 阵列</a>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/virtual/>Lab02 虚拟化实验</a>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/ceph/ class=active>Lab03 Ceph存储集群实践</a>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/table-of-contents/faq/>FAQ</a>
</li>
</ul>
</li>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/resources/>Resources</a>
<ul>
<li>
<a href=https://bugitt.github.io/ns-labs/docs/resources/os/>操作系统</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href=https://github.com/bugitt/ns-labs target=_blank rel=noopener>
Github
</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/ns-labs/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>Lab03 Ceph存储集群实践</strong>
<label for=toc-control>
<img src=/ns-labs/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<nav id=TableOfContents>
<ul>
<li><a href=#lab03-ceph-存储集群实践>Lab03 Ceph 存储集群实践</a>
<ul>
<li><a href=#实验目的>实验目的</a></li>
<li><a href=#实验说明>实验说明</a></li>
<li><a href=#概述>概述</a></li>
<li><a href=#重要概念>重要概念</a>
<ul>
<li></li>
</ul>
</li>
<li><a href=#ceph-部署>Ceph 部署</a>
<ul>
<li><a href=#cephadm>Cephadm</a></li>
<li><a href=#部署前操作>部署前操作</a></li>
<li><a href=#部署>部署</a></li>
</ul>
</li>
<li><a href=#ceph-filesystem-选>Ceph Filesystem (选)</a>
<ul>
<li><a href=#部署-cephfs>部署 CephFS</a></li>
<li><a href=#挂载-cephfs>挂载 CephFS</a></li>
</ul>
</li>
<li><a href=#ceph-rgw-对象存储-选>Ceph RGW 对象存储 (选)</a>
<ul>
<li><a href=#deploy-rgw>Deploy RGW</a></li>
<li><a href=#使用对象存储>使用对象存储</a></li>
</ul>
</li>
<li><a href=#ceph-rbd-选>Ceph RBD (选)</a></li>
<li><a href=#实验报告模板>实验报告模板</a></li>
</ul>
</li>
</ul>
</nav>
</aside>
</header>
<article class=markdown><h1 id=lab03-ceph-存储集群实践>
Lab03 Ceph 存储集群实践
<a class=anchor href=#lab03-ceph-%e5%ad%98%e5%82%a8%e9%9b%86%e7%be%a4%e5%ae%9e%e8%b7%b5>#</a>
</h1>
<h2 id=实验目的>
实验目的
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e7%9b%ae%e7%9a%84>#</a>
</h2>
<ol>
<li>
<p>了解 Ceph 存储的基本工作原理</p>
</li>
<li>
<p>建立对分布式存储的初步认识</p>
</li>
</ol>
<h2 id=实验说明>
实验说明
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e8%af%b4%e6%98%8e>#</a>
</h2>
<p>本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。</p>
<p>当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建 3 台以上的机器进行实验（或者使用其他公有云产品中的机器）。</p>
<p>对于使用自己的机器的同学，建议采用 CentOS 系统（这样 Kernel 版本不至于太新，减少出错的可能），每台机器配置为 2 核 2G 即可，并且每台机器至少附带一个全新的大于 5G 的虚拟磁盘。</p>
<p>请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。</p>
<blockquote class="book-hint info">
<p>Ceph 是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph 也不例外。很多时候，需要对 Linux Kernel 和 Linux 网络管理有基本的了解。</p>
<p><strong>实验中遇到的困难请及时在课程微信群中抛出。</strong></p>
<p>实验文档不可能将整个 Ceph 的文档都搬过来，因此，在实验的整个过程中，请务必将
<a href=https://docs.ceph.com/en/pacific/>Ceph 官方文档</a>也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考
<a href=https://docs.ceph.com/en/pacific/>Ceph 官方文档</a>的相关讨论。</p>
<p>另外，因为 Ceph 是 Red Hat 主导开发的产品，因此，Red Hat 也有关于 Ceph 完整的技术介绍，
<a href=https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5>它的文档</a>同样值得参考，比如，你在这里可以找到它的
<a href=https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/installation_guide/index>Ceph 部署指南</a>，甚至在很多方面比 Ceph 官方更加详实。</p>
</blockquote>
<blockquote class="book-hint danger">
<p>在中文互联网上，你很容易找到
<a href=http://docs.ceph.org.cn/start/intro/>Ceph 的中文文档</a>。但它有很多<strong>过时的内容</strong>，比如它还在使用<code>ceph-deploy</code>这个官方已经不再维护的工具（这个工具同样不支持新版的 Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。</p>
<p>除了中文文档，你还会找到很多良莠不齐的 Blog，这些文章很多也是过时的，在使用时同样务必加以甄别。</p>
<p>在实验过程中，<strong>在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果</strong>。当然，勇敢试错是值得鼓励的，但请做好从头开始的准备。</p>
</blockquote>
<h2 id=概述>
概述
<a class=anchor href=#%e6%a6%82%e8%bf%b0>#</a>
</h2>
<p>Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？</p>
<p>试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。</p>
<p>最初，你将网站部署在一个装有 500G 硬盘的服务器上。随着时间的流逝，500G 的硬盘逐渐被填满。现在你有两种选择。</p>
<ol>
<li>
<p>纵向拓展。在服务器上加装硬盘，甚至你可以使用 LVM 将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。</p>
</li>
<li>
<p>横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。</p>
</li>
</ol>
<p>第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。</p>
<p>但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：</p>
<ul>
<li>
<p>如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？</p>
</li>
<li>
<p>如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。</p>
</li>
<li>
<p>随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是 1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。</p>
</li>
<li>
<p>……</p>
</li>
</ul>
<p>上面这些问题，正是 Ceph 这类分布式存储系统所要解决的问题。简单来说，Ceph 是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供<strong>分布式</strong>、<strong>横向拓展</strong>、<strong>高度可靠性</strong>的存储系统。</p>
<blockquote class="book-hint info">
<p>对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下
<a href=https://pdos.csail.mit.edu/6.824/>MIT 6.824</a>的课程，并尽量完成它的全部实验。</p>
<p>在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B 站上有
<a href=https://www.bilibili.com/video/BV1R7411t71W>翻译好的熟肉</a>。</p>
</blockquote>
<p>除此之外，Ceph 的独特之处还在于，它在一个存储系统上，对外提供了三种类型的访问接口：</p>
<ul>
<li>
<p>文件存储。简单来说，你可以将 Ceph 的存储池抽象为一个文件系统，并挂载到某个目录上，然后像读写本地文件一样，在这个新的目录上创建、读写、删除文件。并且该文件系统可以同时被多台机器同时挂载，并被同时读写。从而实现多台机器间的存储共享。</p>
</li>
<li>
<p>对象存储。Ceph 提供了对象存储网关，并同时提供了 S3 和 Swift 风格的 API 接口。你可以使用这些接口上传和下载文件。</p>
</li>
<li>
<p>块存储。Ceph 还能提供块存储的抽象。即客户端（集群外的机器）通过块存储接口访问的“所有数据按照固定的大小分块，每一块赋予一个用于寻址的编号。”客户端可以像使用硬盘这种块设备一样，使用这些块存储的接口进行数据的读写。（一般这种块设备的读写都是由操作系统代劳的。操作系统会对块设备进行分区等操作，并在其上部署文件系统，应用程序和用户看到直接看到的是文件系统的接口（也就是文件存储））。</p>
</li>
</ul>
<p>
<img src=https://cdn.loheagn.com/123326.jpg alt></p>
<p>需要注意的是，虽然 Ceph 对外提供了上面这三种不同类型的存储接口，但其底层会使用相同的逻辑对接收的数据进行分块和存储。</p>
<h2 id=重要概念>
重要概念
<a class=anchor href=#%e9%87%8d%e8%a6%81%e6%a6%82%e5%bf%b5>#</a>
</h2>
<p>一个 Ceph 集群必须包含三种类型的进程：Monitor、OSD 和 Manager。其中，Monitor 和 OSD 是最核心的两类进程。</p>
<h4 id=monitor-和-osd>
Monitor 和 OSD
<a class=anchor href=#monitor-%e5%92%8c-osd>#</a>
</h4>
<p>Monitor 进程负责维护整个系统的状态信息，这些状态信息包括当前的 Ceph 集群的拓扑结构等，这些信息对 Ceph 集群中各个进程的通信来说非常关键。除此之外，Monitor 进程还负责充当外界（也就是官方文档中总是提到的“Client”）与 OSD 进程交流的媒介。</p>
<p>OSD 进程则负责进行真正的数据存储。如下图所示，外界传送给 Ceph 集群的数据（不管是通过文件存储、对象存储还是块存储的接口）都将被转化为一个个对象（object）。这些 object 将经由 OSD 进程存储到磁盘中。</p>
<p>
<img src=https://cdn.loheagn.com/134513.jpg alt></p>
<p>简单来说，当一个 Client 试图向 Ceph 集群读写数据时，将发生以下步骤：</p>
<ol>
<li>
<p>Client 向 Monitor 进程请求一个 token 校验信息</p>
</li>
<li>
<p>Monitor 生成 token 校验信息，并将其返回给 Client</p>
</li>
<li>
<p>Monitor 同时会将 token 校验信息同步 OSD 进程</p>
</li>
<li>
<p>Client 携带着 Monitor 返回的 token 校验信息向对应的 token 发送数据读写请求</p>
</li>
<li>
<p>OSD 进程将数据存储到合适的位置，或从合适的位置读出数据</p>
</li>
<li>
<p>OSD 进程向 Client 返回数据</p>
</li>
</ol>
<blockquote class="book-hint info">
<p>以上读写数据的流程是经过极致简化的，主要是为了帮助大家建立对 Monitor 进程和 OSD 进程所起的作用的感性认识。</p>
<p>想要了解详情，请阅读
<a href=https://docs.ceph.com/en/pacific/architecture/>Ceph 的文档 - Architecture</a>。</p>
</blockquote>
<h4 id=manager>
Manager
<a class=anchor href=#manager>#</a>
</h4>
<p>Manager 进程主要负责跟踪当前集群的运行时状况，包括当前集群的存储利用率、存储性能等等。同时，它还负责提供 Ceph Dashboard、RESTful 接口的外部服务。</p>
<blockquote class="book-hint info">
<p>需要注意的是，Ceph 集群中有<strong>三类</strong>这样的进程，但不是每个进程只有<strong>一个</strong>。</p>
<p>我们之前提到过，Ceph 是一个有很高容错性的分布式系统，而达到高容错性的一个很重要的方式就是“<strong>冗余</strong>”。</p>
<p>比如，对于 Monitor 进程来讲，集群中仅有一个就够用了。但如果运行这一个 Monitor 进程的机器挂了，那么整个集群就会瘫痪（Client 将不知道该跟谁通信来拿到校验信息和集群状态信息等）。因此，一个高可用的 Ceph 集群中会包含多个执行几乎相同任务的运行在不同机器上的 Monitor 进程；这样挂了一个，Client 还可以跟剩下的通信，整个集群依旧可以正常对外提供服务。同样的道理，OSD 进程和 Manager 进程也有多个副本。</p>
<p>另外，Ceph 为了保证数据的可靠性（也就是说 Client 存储进来的数据不能丢失）——注意区分其与整个系统可靠性的区别——在默认情况下，会将每份数据存储<strong>3 份</strong>，每份都会存储在不同的 OSD 上（鸡蛋不能放到同一个篮子里）。这样，即使有部分 OSD 挂掉，也能保证大部分数据不会丢失。因此，一个健康的 Ceph 集群要求至少同时存在三个健康的 OSD 进程（当然，这个默认的数值可以更改）。</p>
</blockquote>
<h4 id=pool-与-placement-group-pg-与-placement-group-for-placement-purpose-pgp>
Pool 与 Placement Group (PG) 与 Placement Group for Placement purpose (PGP)
<a class=anchor href=#pool-%e4%b8%8e-placement-group-pg-%e4%b8%8e-placement-group-for-placement-purpose-pgp>#</a>
</h4>
<p>请查阅 Ceph 的相关文档，阐述 Pool、PG、PGP 与 OSD 之间的关系。可定性/定量分析 OSD、PG、PGP、OSD、PG_NUM 之间的数量关系。</p>
<h4 id=ceph-集群的-health-status>
Ceph 集群的 HEALTH STATUS
<a class=anchor href=#ceph-%e9%9b%86%e7%be%a4%e7%9a%84-health-status>#</a>
</h4>
<p>你在实验过程中，打 <code>ceph -s</code> 都遇到过哪些 HEALTH STATUS？（<code>ceph health detail</code> 能看到更为详细的状态信息）如果是 <code>WARN/ERROR</code> 都是哪些原因？</p>
<h4 id=pg_num-的相关计算>
PG_NUM 的相关计算
<a class=anchor href=#pg_num-%e7%9a%84%e7%9b%b8%e5%85%b3%e8%ae%a1%e7%ae%97>#</a>
</h4>
<p>请查阅 Ceph 的相关文档，</p>
<h2 id=ceph-部署>
Ceph 部署
<a class=anchor href=#ceph-%e9%83%a8%e7%bd%b2>#</a>
</h2>
<p>本节内容的目标是创建一个可用的 Ceph 集群。其中包括，至少一个 Monitor 进程、至少一个 Manager 进程、至少三个 OSD 进程。</p>
<blockquote class="book-hint info">
<p>使用云平台提供的机器进行实验的同学，请直接使用 root 用户登录，密码是<code>&shieshuyuan21</code>。</p>
<p>登录后，请首先使用<code>passwd</code>命令修改密码。</p>
</blockquote>
<p>Ceph 官方提供了
<a href=https://docs.ceph.com/en/pacific/install/>多种部署方式</a>。</p>
<p>在本实验文档中，我们采用
<a href=https://docs.ceph.com/en/pacific/cephadm/#cephadm>Cephadm</a>作为部署工具。Cephadm 也是官方推荐的部署和管理 Ceph 集群的工具，它不仅可以用来部署 Ceph，还可以在安装完成后，用来管理集群（添加和移除节点、开启 rgw 等）。</p>
<p>当然，如果你对其他部署方式感兴趣的话，也可以尝试；比如，对 Kubernetes 比较感兴趣的同学，可以尝试使用
<a href=https://rook.io/>Rook</a>进行部署。</p>
<h3 id=cephadm>
Cephadm
<a class=anchor href=#cephadm>#</a>
</h3>
<p>Cephadm 是基于“容器技术（Container）”进行工作的，每个 Ceph 的工作进程都运行在相互隔离的容器中。Cephadm 支持使用 Docker 和 Podman 作为容器运行时。在部署时，Cephadm 将首先检查本机中安装的容器运行时类型，当 Docker 与 Podman 并存时，将首先使用 Podman（毕竟 Podman 也是 Red Hat 的产品）。</p>
<p>在部署时，Cephadm 会首先在本地启动一个 mini 的 Ceph 集群，其中包括 Ceph 集群最基本的 Monitor 进程和 Manager 进程（当然，这两个进程都是通过容器形式运行起来的）。这个 mini 集群在某种程度上来说也是合法的，只不过其基本不能对外提供任何功能。随后，我们将继续使用 Cephadm 提供的工具，将其他机器（后文中也会称之为“节点”）加入到集群中（也就是在其他节点中启动 Ceph 的 Monitor、Manager、OSD 等进程），从而构建一个完整可用的集群。</p>
<p>下面是 Red Hat 给出使用 Cephadm 构建的集群的架构图。</p>
<p>
<img src=https://cdn.loheagn.com/090719.jpg alt></p>
<p>图中的“Container”指的就是我们上面所说的“容器”。</p>
<p>图中最左边的 Bootstrap Host 就是我们执行 Cephadm 相关命令的机器（事实上，在整个集群的构建过程中，除了修改 IP 和联网等操作外，我们都只会在这个 Bootstrap Host 上进行操作）。我们在 Bootstrap Host 执行的针对其他节点的操作，都是 Cephadm 通过 ssh 的方式发送给对应节点的。</p>
<p>这个图目前大家心里有个印象就好，在后续的实验操作中，可以反复回来对照检查。</p>
<h3 id=部署前操作>
部署前操作
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e5%89%8d%e6%93%8d%e4%bd%9c>#</a>
</h3>
<h4 id=联网>
联网
<a class=anchor href=#%e8%81%94%e7%bd%91>#</a>
</h4>
<p>部署过程中的操作尽量都联网进行。云平台的 Linux 机器联网操作请参考：</p>
<blockquote>
<p>推荐使用
<a href=https://github.com/Dr-Bluemond/srun>Dr-Bluemond/srun</a>提供的工具。云平台提供已经编译好的 Linux64 位版本。可以这样获取：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>wget https://scs.buaa.edu.cn/scsos/tools/linux/buaalogin
chmod +x ./buaalogin
</code></pre></div><p>使用前请使用<code>config</code>命令配置一下校园网用户名和密码（注意，如果用户名中有英文的话，请大小写都尝试一下）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./buaalogin config
</code></pre></div><p>配置完成后，使用<code>login</code>命令登录即可：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./buaalogin login
</code></pre></div><p>或，直接：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./buaalogin
</code></pre></div><p>如果想作为系统命令使用的话（注意替换合适的安装路径）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo install ./buaalogin /usr/local/bin
</code></pre></div></blockquote>
<h4 id=安装-cephadm>
安装 Cephadm
<a class=anchor href=#%e5%ae%89%e8%a3%85-cephadm>#</a>
</h4>
<blockquote class="book-hint info">
对于云平台提供的虚拟机，Cephadm 相关的工具已经安装好，本步骤可以略过。
</blockquote>
<blockquote class="book-hint info">
本步骤需要在各台机器上都要完成
</blockquote>
<p>Cephadm 也是个命令行工具，在使用前也需要额外安装这个工具。</p>
<ol>
<li>
<p>保证系统已经安装好 curl、Python3、Podman/Docker(推荐使用 Podman，都是自家 RedHat 的产品)</p>
</li>
<li>
<p>下载 Cephadm：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl --silent --remote-name --location https://scs.buaa.edu.cn/scsos/ceph/cephadm
</code></pre></div><p>赋予可执行权限；</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>chmod +x ./cephadm
</code></pre></div></li>
<li>
<p>安装相关工具（请务必保持联网状态，可能会耗费比较久的时间，请耐心等待）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./cephadm add-repo --release octopus
./cephadm install
cephadm install ceph-common
</code></pre></div></li>
</ol>
<h4 id=配置静态-ip修改-hostname>
配置静态 IP，修改 Hostname
<a class=anchor href=#%e9%85%8d%e7%bd%ae%e9%9d%99%e6%80%81-ip%e4%bf%ae%e6%94%b9-hostname>#</a>
</h4>
<blockquote class="book-hint info">
本步骤需要在各台机器上都要完成
</blockquote>
<p>Ceph 集群的各个机器之间是通过 IP 地址进行通信的，而云平台的机器在重启后通过 DHCP 获取的 IP 可能会发生变化，这将给实验带来一定的干扰。</p>
<p>因此，建议在正式开始实验前，为机器配置静态 IP（即，将你此时的虚拟机的 IP 固定下来，即禁用 DHCP，改为手动指定 IP、网关、DNS 服务器等）。对于使用个人机器进行实验的同学，同样也建议配置一下静态 IP。</p>
<blockquote class="book-hint info">
使用云平台提供的机器的同学，请参考下方给出的 CentOS 机器的配置方法。
</blockquote>
<div class=book-tabs><input type=radio class=toggle name="tabs-Static IP" id="tabs-Static IP-0" checked>
<label for="tabs-Static IP-0">CentOS</label>
<div class="book-tabs-content markdown-inner"><p>对于 CentOS 的机器，配置 IP 可以手动编辑<code>/etc/sysconfig/network-scripts/</code>下的配置文件。</p>
<p>也可以采用 NetworkManager 提供的下面这种图形化的终端配置方法。</p>
<p>登录到虚拟机后，使用<code>nmtui</code>编辑需要配置静态 IP 的网卡：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nmtui edit ens150
</code></pre></div><blockquote class="book-hint danger">
<h4 id=指定正确的网卡>
指定正确的网卡
<a class=anchor href=#%e6%8c%87%e5%ae%9a%e6%ad%a3%e7%a1%ae%e7%9a%84%e7%bd%91%e5%8d%a1>#</a>
</h4>
<p>上面这条命令中的<code>ens150</code>指的是你要指定静态 IP 的网卡名称。请根据自己的实际情况指定网卡名称。</p>
<p>如何查看自己的网卡名称？</p>
<p>在终端中输入<code>ip a</code>，将得到大致如下的输出：</p>
<p>
<img src=https://cdn.loheagn.com/150722.png alt></p>
<p>可以看到，这里列出了当前机器上的所有网卡及其上的 IP 等信息，并且每个都有编号。</p>
<p>找到那个标有你的 IP 地址的网卡信息，即可找到它的网卡网卡名称。</p>
<p>比如，在上图中，你要固定的 IP 是<code>10.251.253.10</code>，找到它对应的那一组网卡信息，即可看到网卡名称是<code>ens160</code>。</p>
</blockquote>
<p>在打开的如下图所示的界面中，编辑<code>IPv4 CONFIGURATION</code>下面的几项。</p>
<p>
<img src=https://cdn.loheagn.com/064525.png alt></p>
<p>首先，将<code>Automatic</code>切换为<code>Manual</code>。意思是，不希望通过 DHCP 自动获取 IP，而是手动指定 IP。</p>
<p>然后，编辑<code>Address</code>，填入你当前的 IP 地址，并给出子网掩码的位数。对于云平台的机器子网掩码位数是<code>22</code>；使用本地机器的同学，请根据实际情况配置。</p>
<p>然后，编辑<code>Gateway</code>，即网关地址。对于云平台的机器，这一地址是<code>10.251.252.1</code>；使用本地机器的同学，请根据实际情况配置。</p>
<p>然后，编辑<code>DNS servers</code>，即 DNS 服务器地址。对于校园网内的机器，都可以将 DNS 地址指定为<code>202.112.128.50</code>。</p>
<p>之后，选择最下面的<code>OK</code>退出即可。</p>
<p>总而言之，如果使用云平台的机器，最终配置的结果与下图所示应该一直（仅 IP 地址不同）。</p>
<p>
<img src=https://cdn.loheagn.com/150020.png alt></p>
</div><input type=radio class=toggle name="tabs-Static IP" id="tabs-Static IP-1">
<label for="tabs-Static IP-1">Debian 及 Ubuntu 16.04</label>
<div class="book-tabs-content markdown-inner"><p>直接编辑 <code>/etc/network/interfaces</code> 文件即可。具体可以参考
<a href=https://michael.mckinnon.id.au/2016/05/05/configuring-ubuntu-16-04-static-ip-address/>这篇文章</a>。</p>
<p>其中，对于云平台的机器，需要指定<code>gateway</code>为<code>10.251.253.10</code>，<code>netmask</code>为<code>255.255.252.0</code>，<code>dns-nameservers</code>为<code>202.112.128.50</code>。</p>
</div><input type=radio class=toggle name="tabs-Static IP" id="tabs-Static IP-2">
<label for="tabs-Static IP-2">Ubuntu 18.04 及以上</label>
<div class="book-tabs-content markdown-inner"><p>Ubuntu 18.04 开始，Ubuntu 开始使用 Netplan 来管理网络连接。因此，配置静态 IP 需要编辑 <code>/etc/netplan/</code> 下的配置文件。具体可以参考
<a href=https://linuxize.com/post/how-to-configure-static-ip-address-on-ubuntu-18-04/>这篇文章</a>。</p>
<p>其中，对于云平台的机器，需要指定<code>gateway</code>为<code>10.251.253.10</code>，<code>netmask</code>为<code>255.255.252.0</code>，<code>nameservers</code>为<code>202.112.128.50</code>。</p>
</div></div>
<blockquote class="book-hint danger">
<p>对于使用云平台的机器的同学，配置静态 IP 时，请<strong>务必务必</strong>保证使用现有的 IP。</p>
<p>也就是说，配置静态 IP 前后，你的机器的 IP 应该始终相同。</p>
</blockquote>
<p>配置完静态 IP 后，还需要修改一下机器的 hostname，以区分集群内的各台机器。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>echo <span style=color:#e6db74>&#39;你的机器的Hostname&#39;</span> &gt; /etc/hostname
</code></pre></div><p>机器的 hostname 其实可以任意指定，但最好有意义，并携带上你的学号，以防止云平台上几十台实验虚拟机的 hostname 发生冲突。</p>
<p>比如，如果你的学号是<code>15213304</code>，可以将第一台机器的 hostname 指定为<code>ceph-01-15213304</code>，第二台机器的 hostname 指定为<code>ceph-02-15213304</code>。</p>
<p>配置完静态 IP 和 hostname 后，请使用 <code>reboot</code> 命令重启机器。</p>
<h3 id=部署>
部署
<a class=anchor href=#%e9%83%a8%e7%bd%b2>#</a>
</h3>
<blockquote class="book-hint info">
基于 Cephadm 的便捷性，部署部分的操作只需要在<strong>选定的一台</strong>机器（我们把这台机器称为 Bootstrap Host 上执行即可。
</blockquote>
<blockquote class="book-hint info">
在进行部署操作前，请务必保证所有机器处于联网状态。
</blockquote>
<h4 id=bootstrap>
BootStrap
<a class=anchor href=#bootstrap>#</a>
</h4>
<p>我们首先需要在一台选定的机器上，使用<code>cephadm</code>启动一个 mini 集群。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cephadm --image harbor.scs.buaa.edu.cn/ceph/ceph:v16 bootstrap  --mon-ip *&lt;mon-ip&gt;*
</code></pre></div><p>请将<code>*&lt;mon-ip>*</code>替换为你执行这命令的机器的 IP。</p>
<p>如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cephadm --image harbor.scs.buaa.edu.cn/ceph/ceph:v16 bootstrap --mon-ip 10.251.219.248
</code></pre></div><p>上面这条命令中，<code>--image</code>制定了 Cephadm 启动容器时使用的镜像名称，<code>--mon-ip</code>指定了 Cephadm 要在哪个机器上启动一个 mini 集群。</p>
<p>更详细地，这条命令将会做如下事情：</p>
<blockquote>
<ul>
<li>
<p>Create a monitor and manager daemon for the new cluster on the local host.</p>
</li>
<li>
<p>Generate a new SSH key for the Ceph cluster and add it to the root user&rsquo;s /root/.ssh/authorized_keys file.</p>
</li>
<li>
<p>Write a copy of the public key to /etc/ceph/ceph.pub.</p>
</li>
<li>
<p>Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster.</p>
</li>
<li>
<p>Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring.</p>
</li>
<li>
<p>Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring.</p>
</li>
</ul>
</blockquote>
<p>命令执行完成后，我们可以通过<code>ceph -s</code>查看当前集群的状态。</p>
<p>
<img src=https://cdn.loheagn.com/074508.png alt></p>
<p>可以看到，确实启动了一个 Monitor 进程和一个 Manager 进程。</p>
<p>另外，我们还注意到，当前集群的健康状态是<code>HEALTH_WARN</code>，原因下面也列出来了：<code>OSD count 0 &lt; osd_pool_default_size 3</code>。这是因为当前 Ceph 集群默认的每个 Pool 的副本数应该是 3（即，Ceph 中存储的每份数据必须复制 3 份，放在 3 个不同的 OSD 中），但我们 OSD 的进程数是 0。不用着急，马上我们就会创建足够的 OSD 进程（Bootstrap Host、另外两台主机都为他们创建 OSD 进程）。</p>
<p>
<img src=https://cdn.loheagn.com/074750.png alt></p>
<p>mini 集群启动完成后，可以使用<code>podman ps</code>查看当前启动的容器的状态：</p>
<p>
<img src=https://cdn.loheagn.com/091703.png alt></p>
<p>可以看到，最上面那两个容器，对应的就是 ceph 集群的 Monitor 进程和 Manager 进程。</p>
<h4 id=ceph-dashboard-选>
Ceph Dashboard (选)
<a class=anchor href=#ceph-dashboard-%e9%80%89>#</a>
</h4>
<p>注意看<code>bootstrap</code>指令的输出，你可以看到一段这样的内容：</p>
<p>
<img src=https://cdn.loheagn.com/074911.png alt></p>
<p>显然，这是在告诉我们，cephadm 同样启动了一个<code>Ceph Dashboard</code>，这是一个 Ceph 的管理前端。通过访问这个页面，我们就可以以可视化的方式观察到当前集群的状态。</p>
<p>这段信息中的<code>ceph-01</code>代指的就是你当前这台机器的 IP（当然，如果你的 Hostname 不是<code>ceph-01</code>的话，得到的将是其他的结果）。用你的机器的 IP 替换这个<code>ceph-01</code>，尝试在浏览器访问它。</p>
<p>是不是访问失败了？这是因为 Ceph Dashboard 默认使用的是 HTTPS 协议，而且它用的 HTTPS 证书还是自己签发的。。。为了能正常访问，我们可以手动禁用 SSL：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph config set mgr mgr/dashboard/ssl false
</code></pre></div><p>但禁用 SSL 后，Dashboard 服务将默认监听 8080 端口。但在 CentOS 中，8080 端口默认是被防火墙屏蔽的。</p>
<p>你可以选择手动打开防火墙的 8080 端口；也可以像下面这样，将 Dashboard 服务的监听端口手动改为 8443（因为这个端口就是使用 HTTPS 时 Dashboard 的监听端口，在刚才的 Bootstrap 时已经在防火墙中打开了）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph config set mgr mgr/dashboard/server_port <span style=color:#ae81ff>8443</span>
</code></pre></div><p>然后，重启该 Dashboard 服务，使刚才的配置生效。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph mgr module disable dashboard
ceph mgr module enable dashboard
</code></pre></div><p>然后，查看当前的服务状态（如果输出为空的话，耐心多等一会儿）：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph mgr services
</code></pre></div><p>
<img src=https://cdn.loheagn.com/080831.png alt></p>
<p>现在，你应该可以正常访问 Dashboard 服务了。注意，用户名和密码是我们前面提到的 Bootstrap 命令输出的那堆信息中提到的。</p>
<p>
<img src=https://cdn.loheagn.com/081143.png alt></p>
<h4 id=添加其他节点>
添加其他节点
<a class=anchor href=#%e6%b7%bb%e5%8a%a0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9>#</a>
</h4>
<blockquote class="book-hint danger">
添加其他节点前，请保证所有节点都处于联网状态
</blockquote>
<p>接下来，我们将把其他机器添加到现有的这个 mini 集群中来。</p>
<p>前面提到过，cephadm 是通过 ssh 协议与其他机器通信的。所以，这里需要首先把 Bootstrap Host 机器的公钥 copy 到其他的所有机器：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh-copy-id -f -i /etc/ceph/ceph.pub root@*&lt;new-host&gt;*
</code></pre></div><p>例如，如果你的有一台机器的 IP 是<code>10.252.252.40</code>，那么这条命令应该是：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh-copy-id -f -i /etc/ceph/ceph.pub root@10.252.252.40
</code></pre></div><p>处理完所有的机器后，就可以正式将它们加入到 mini 集群中来了：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch host add *&lt;newhost&gt;* <span style=color:#f92672>[</span>*&lt;ip&gt;*<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>*&lt;label1&gt; ...*<span style=color:#f92672>]</span>
</code></pre></div><p>例如，你有一台机器的 Hostname 是<code>ceph-02</code>，IP 是<code>10.252.252.40</code>，那么这条命令应该是：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch host add ceph-02 10.252.252.40
</code></pre></div><p>添加完成后，你可以通过<code>ceph -s</code>查看当前集群状态的变化。也可以通过 Ceph Dashboard 看到变化。</p>
<h4 id=创建-osd-进程>
创建 OSD 进程
<a class=anchor href=#%e5%88%9b%e5%bb%ba-osd-%e8%bf%9b%e7%a8%8b>#</a>
</h4>
<p>我们知道，OSD 进程是真正用来做数据读写的进程。我们可以用一块专门的磁盘交给 OSD 进程来读写数据，ceph 集群所存储的数据就将保存在这些磁盘中。</p>
<p>这些被用来交给 OSD 进程管理的磁盘，应该满足以下条件：</p>
<blockquote>
<ul>
<li>
<p>The device must have no partitions.</p>
</li>
<li>
<p>The device must not have any LVM state.</p>
</li>
<li>
<p>The device must not be mounted.</p>
</li>
<li>
<p>The device must not contain a file system.</p>
</li>
<li>
<p>The device must not contain a Ceph BlueStore OSD.</p>
</li>
<li>
<p>The device must be larger than 5 GB.</p>
</li>
</ul>
</blockquote>
<p>简单来说，就是将一块干净的磁盘插入机器后，什么都不用做就好。</p>
<p>在云平台分配的虚拟机中，每台机器都额外插入了一块这样干净的磁盘。如是在自己本机上做，可在 VMware 中添加磁盘，重启后即可看到新添加的干净磁盘 sdb。</p>
<p>可以通过<code>fdisk -l</code>来查看：</p>
<p>
<img src=https://cdn.loheagn.com/090352.png alt></p>
<p>注意看上面这两块磁盘：</p>
<ul>
<li>
<p>第一个名称是<code>/dev/sda</code>，容量是 16G，有两个分区：<code>/dev/sda1</code>，<code>/dev/sda2</code>。这就是我们现在在使用的这个系统所用的磁盘，系统数据都存储在这个磁盘中。</p>
</li>
<li>
<p>第二个名称是<code>/dev/sdb</code>，容量是 10G，没有任何分区。这就是我们即将交给 OSD 管理的磁盘。</p>
</li>
</ul>
<p>使用下面的命令来创建 OSD 进程：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch daemon add osd *&lt;hostname&gt;*:*&lt;device-name&gt;*
</code></pre></div><p>比如，你要在主机<code>ceph-01</code>的名称为<code>/dev/sdb</code>的磁盘上创建 OSD 进程，那么命令应该是：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch daemon add osd ceph-01:/dev/sdb
</code></pre></div><blockquote class="book-hint info">
<p>这条命令默认不会有输出创建 OSD 进程的详细信息，也就是说，如果该命令很耗时的话，那么你将在什么输出都没有的情况下等待较长时间，这可能令人发慌。你可以加上<code>--verbose</code>参数（缩写为<code>-v</code>），来让它输出详细信息。</p>
<p>比如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch daemon add osd ceph-01:/dev/sdb --verbose
</code></pre></div>
</blockquote>
<p>执行完成后，可以使用<code>ceph -s</code>查看当前的集群状态，可以发现已经有一个 OSD 进程加入了进来。</p>
<p>
<img src=https://cdn.loheagn.com/105544.png alt></p>
<p>使用同样的方法，将所有节点的附加硬盘都加入进来。至此，我们再使用<code>ceph -s</code>查看当前集群的状态，应为<code>HEALTH_OK</code>。</p>
<h2 id=ceph-filesystem-选>
Ceph Filesystem (选)
<a class=anchor href=#ceph-filesystem-%e9%80%89>#</a>
</h2>
<p>
<a href=https://docs.ceph.com/en/latest/cephfs/>参考资料: CEPH FILE SYSTEM</a></p>
<p>Ceph 文件系统 (Ceph FS)是个 POSIX 兼容的文件系统，它使用 Ceph 存储集群来存储数据。 Ceph 文件系统与 Ceph 块设备、同时提供 S3 和 Swift API 的 Ceph 对象存储、或者原生库 (librados) 一样，都使用着相同的 Ceph 存储集群系统。</p>
<p>Ceph 文件系统要求 Ceph 存储集群内至少有一个 Ceph 元数据服务器 MDS。</p>
<h3 id=部署-cephfs>
部署 CephFS
<a class=anchor href=#%e9%83%a8%e7%bd%b2-cephfs>#</a>
</h3>
<p>以上其实都是在搭建 Ceph 集群的环境，我们添加了 3 个 OSD 进程组建起了 Ceph Cluster。接下来，我们便可以在此基础上来具体地使用到 Ceph 所提供的分布式存储能力，从 Ceph 文件系统 CephFS 开始~</p>
<div class=book-tabs><input type=radio class=toggle name="tabs-Deploy CephFS" id="tabs-Deploy CephFS-0" checked>
<label for="tabs-Deploy CephFS-0">Automatic Setup</label>
<div class="book-tabs-content markdown-inner"><p>创建 CephFS 的前提是需要至少一个的 MDS daemon 元数据服务器守护进程。其实以下的一条命令即可自动地创建好 MDS daemon、Pool 等：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph fs volume create &lt;fs_name&gt; <span style=color:#f92672>[</span>--placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;placement spec&gt;&#34;</span><span style=color:#f92672>]</span>
</code></pre></div><p>其中，<code>fs_name</code> 是 CephFS 的名称，后面的 <code>--placement</code> 为可选参数，可以通过它来指定 daemon container 跑在哪几个 hosts 上。
<a href=https://docs.ceph.com/en/latest/cephfs/fs-volumes/>参考资料</a></p>
</div><input type=radio class=toggle name="tabs-Deploy CephFS" id="tabs-Deploy CephFS-1">
<label for="tabs-Deploy CephFS-1">More Customized Setup</label>
<div class="book-tabs-content markdown-inner"><p>
<a href=https://amito.me/2018/Pools-and-Placement-Groups-in-Ceph/>参考：下列命令的一些具体参数含义</a></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph osd pool create cephfs_data <span style=color:#ae81ff>8</span> <span style=color:#ae81ff>8</span> <span style=color:#75715e># 后面的数量可以调，设大了会无法创建，数值和osd的数量有关，需要是2的倍数</span>
<span style=color:#75715e># pool &#39;cephfs_data&#39; created</span>

ceph osd pool create cephfs_metadata <span style=color:#ae81ff>8</span> <span style=color:#ae81ff>8</span>
<span style=color:#75715e># pool &#39;cephfs_metadata&#39; created</span>

ceph fs new cephfs cephfs_metadata cephfs_data
<span style=color:#75715e># new fs with metadata pool 3 and data pool 2</span>

ceph fs ls
<span style=color:#75715e># name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span>

ceph orch apply mds cephfs --placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;3 node1 node2 node3&#34;</span> <span style=color:#75715e># 应用部署CephFS</span>
</code></pre></div></div></div>
<p>于是，我们便创建成功了 CephFS。</p>
<p>在上述方法中，我们还经常看到有 <code>--placement</code>的可选参数，我们可以直接去输各 host 的名字，也可以为每个 host 加 label 标签来进行指定：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch host ls <span style=color:#75715e># 查看当前有的hosts</span>
<span style=color:#75715e># 可有类似输出</span>
HOST   ADDR      LABELS  STATUS
host1  10.1.2.3
host2  10.1.2.4
host3  10.1.2.5
</code></pre></div><p>但他们都没有 Label 标签，我们可通过 <code>ceph orch host label add host1 mylabel</code> 添加指定的 Label。</p>
<p>则 <code>[--placement="&lt;placement spec>"]</code> 可为 <code>--placement="label:mylabel"</code></p>
<p>想折腾的可能会想到如何删除 CephFS，Ceph 中非常“贴心”地防止你误删除，所以删除起来会有一些麻烦。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph fs volume rm &lt;fs_name&gt; <span style=color:#f92672>[</span>--yes-i-really-mean-it<span style=color:#f92672>]</span> <span style=color:#75715e># 要加上这么一长串后缀，但这样其实还是没法删除</span>
ceph config set mon mon_allow_pool_delete true <span style=color:#75715e># 还需要通过这条命令修改ceph config配置</span>
</code></pre></div><blockquote class="book-hint info">
<ul>
<li>通过 <code>rados df</code> 命令可查看刚才创建的资源池 Pool 的相关信息。</li>
<li><code>ceph fs ls</code> 可列出 CephFS。</li>
<li><code>ceph fs status</code> 可查看 CephFS 状态，验证当前已有至少一个 MDS 处在 Active 状态。</li>
<li>还可经常性地执行 <code>ceph -s</code> 查看 Ceph 集群的状态，确保一直处在 <code>HEALTH_OK</code>。</li>
</ul>
</blockquote>
<h3 id=挂载-cephfs>
挂载 CephFS
<a class=anchor href=#%e6%8c%82%e8%bd%bd-cephfs>#</a>
</h3>
<p>
<a href=https://people.redhat.com/bhubbard/nature/default/cephfs/fuse/>参考资料：MOUNT CEPHFS USING FUSE</a></p>
<p>CephFS 在创建后应当能被实际使用，如完成分布式存储文件的任务。在这一步，我们将把 CephFS 挂载到 Client 端，让 Client 能够创建和存储文件。</p>
<p>我们先要对 Client 端进行一些配置，保证 Client 端能连接到 MON 主机，即 Bootstrap Host。</p>
<p>第一步：Generate a minimal conf for the client host. The conf file should be placed at /etc/ceph:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># on client host</span>
mkdir /etc/ceph
ssh <span style=color:#f92672>{</span>user<span style=color:#f92672>}</span>@<span style=color:#f92672>{</span>mon-host<span style=color:#f92672>}</span> <span style=color:#e6db74>&#34;sudo ceph config generate-minimal-conf&#34;</span> | sudo tee /etc/ceph/ceph.conf
chmod <span style=color:#ae81ff>644</span> /etc/ceph/ceph.conf <span style=color:#75715e># 赋权</span>
</code></pre></div><p>如果不能成功，可直接到 MON 主机执行<code>sudo ceph config generate-minimal-conf</code>，将输出的内容粘贴到 <code>/etc/ceph/ceph.conf</code>（下同）。</p>
<p>第二步：Create the CephX user and get its secret key:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># on client host</span>
ssh <span style=color:#f92672>{</span>user<span style=color:#f92672>}</span>@<span style=color:#f92672>{</span>mon-host<span style=color:#f92672>}</span> <span style=color:#e6db74>&#34;sudo ceph fs authorize cephfs client.foo / rw&#34;</span> | sudo tee /etc/ceph/ceph.client.foo.keyring
chmod <span style=color:#ae81ff>600</span> /etc/ceph/ceph.client.foo.keyring <span style=color:#75715e># 赋权</span>
</code></pre></div><p>在上述命令中，cephfs 是先前所创建的 CephFS 的名称，请将其替换。foo 是 CephX 的用户名，也可自己起。</p>
<p>以上是前置准备，完成后，我们可通过 ceph-fuse 工具实现文件挂接。如机器上没有，则需要连网安装一下。</p>
<p>安装完成后，我们可以创建一个被挂接的目录，如 <code>mycephfs</code>：<code>mkdir /mnt/mycephfs</code></p>
<p>执行 <code>ceph-fuse -id foo /mnt/mycephfs</code> 即可完成挂接。</p>
<p>如上条命令无法成功运行，可从
<a href=https://people.redhat.com/bhubbard/nature/default/cephfs/fuse/>参考资料：MOUNT CEPHFS USING FUSE</a>试一下其他的命令，如 <code>ceph-fuse --id foo -m 192.168.0.1:6789 /mnt/mycephfs</code>，进一步指定了 MON 进程的 IP 和端口号。</p>
<p>若想取消挂接非常简单，只需 <code>umount /mnt/mycephfs</code>。</p>
<blockquote class="book-hint info">
<p><strong>如何判断挂接成功？</strong></p>
<p>上述命令不报错是一方面，我们也可以通过一些命令来看挂接的情况。</p>
<ul>
<li><code>lsblk</code> 列出所有可用块设备的信息，还能显示他们之间的依赖关系</li>
<li><code>df -h</code> 查看磁盘占用的空间</li>
</ul>
<p>通过这些命令，应能看到挂接盘 mycephfs 的存在，查看到其容量等信息。</p>
</blockquote>
<h2 id=ceph-rgw-对象存储-选>
Ceph RGW 对象存储 (选)
<a class=anchor href=#ceph-rgw-%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8-%e9%80%89>#</a>
</h2>
<p>Ceph RGW(即 RADOS Gateway)是 Ceph 对象存储网关服务，是基于 LIBRADOS 接口封装实现的 FastCGI 服务，对外提供存储和管理对象数据的 Restful API。对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前 Ceph RGW 兼容常见的对象存储 API，例如兼容绝大部分 Amazon S3 API，兼容 OpenStack Swift API。</p>
<p>通俗理解是 RGW 作为一个协议转换层，把从上层应用符合 S3 或 Swift 协议的请求转换成 rados 的请求，将数据保存在 rados 集群中。</p>
<p>
<img src=https://docs.ceph.com/en/octopus/_images/1ae399f8fa9af1042d3e1cbf31828f14eb3fe01a6eb3352f88c3d2a04ac4dc50.png alt=rgw></p>
<blockquote class="book-hint info">
<p><strong>内部概念</strong></p>
<ul>
<li>zone：包含多个 RGW 实例的一个逻辑概念。zone 不能跨集群，同一个 zone 的数据保存在同一组 pool 中。</li>
<li>zonegroup：一个 zonegroup 如果包含 1 个或多个 zone。如果一个 zonegroup 包含多个 zone，必须指定一个 zone 作为 master</li>
<li>zone，用来处理 bucket 和用户的创建。一个集群可以创建多个 zonegroup，一个 zonegroup 也可以跨多个集群。</li>
<li>realm：一个 realm 包含 1 个或多个 zonegroup。如果 realm 包含多个 zonegroup，必须指定一个 zonegroup 为 master</li>
<li>zonegroup， 用来处理系统操作。一个系统中可以包含多个 realm，多个 realm 之间资源完全隔离。</li>
</ul>
<p><strong>外部概念</strong></p>
<ul>
<li>user：对象存储的使用者，默认情况下，一个用户只能创建 1000 个存储桶。</li>
<li>bucket：存储桶，用来管理对象的容器。</li>
<li>object：对象，泛指一个文档、图片或视频文件等，尽管用户可以直接上传一个目录，但是 ceph 并不按目录层级结构保存对象， ceph 所有的对象扁平化的保存在 bucket 中。</li>
</ul>
<p>
<a href=https://durantthorvalds.top/2021/01/03/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E4%B8%83%EF%BC%9A%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3RGW/>参考阅读</a></p>
</blockquote>
<h3 id=deploy-rgw>
Deploy RGW
<a class=anchor href=#deploy-rgw>#</a>
</h3>
<p>
<a href=https://docs.ceph.com/en/latest/cephadm/services/rgw/>参考：RGW SERVICE</a></p>
<ul>
<li>To deploy a set of radosgw daemons, with an arbitrary service name name, run the following command:</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph orch apply rgw *&lt;name&gt;* <span style=color:#f92672>[</span>--realm<span style=color:#f92672>=</span>*&lt;realm-name&gt;*<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>--zone<span style=color:#f92672>=</span>*&lt;zone-name&gt;*<span style=color:#f92672>]</span> --placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;*&lt;num-daemons&gt;* [*&lt;host1&gt;* ...]&#34;</span>
</code></pre></div><p>其中，<code>[]</code>中的内容为可选项，可以都添上。如 <code>ceph orch apply rgw *&lt;name>* --realm=default --zone=default --placement=3</code>。<code>--placement</code>参数的使用和先前实验也是类似的，还可以用 <code>3 node1 node2 node3</code> 来完成指定，以及可以通过 Label 来指定。</p>
<p>其实这一条命令就够了，然后我们可查看各个 rgw 节点是否已启动：<code>ceph orch ps --daemon-type rgw</code>。</p>
<p>应能看到 <code>rgw*</code> 均为 <code>running</code> 的 STATUS，则表明顺利启动。若为 <code>starting</code> 可稍等其转为 <code>running</code>。</p>
<blockquote class="book-hint info">
<p>如一直显示 unknown/error 的 STATUS，也是因为 pg 资源不足（受 osd 数量所限）导致的，可以将前面创建的 CephFS 删掉，释放资源。</p>
<p>删除刚刚创建的 unknown/error 的 rgw 的命令为<code>ceph orch rm rgw.*&lt;rgw_name>*</code></p>
</blockquote>
<p>在执行上述命令的 Bootstrap host，<code>curl &lt;bootstrap_host_ip:80></code> 应能看到包含了 <code>&lt;Buckets/></code> 的 XML 形式的输出。</p>
<h3 id=使用对象存储>
使用对象存储
<a class=anchor href=#%e4%bd%bf%e7%94%a8%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8>#</a>
</h3>
<p>我们为 rgw 创建用户：<code>radosgw-admin user create --uid=&lt;username> --display-name=&lt;your_display_name> --system</code>。如 <code>radosgw-admin user create --uid=s3 --display-name="objcet_storage" --system</code>。执行后，能看到类似输出：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:#f92672>&#34;user_id&#34;</span>: <span style=color:#e6db74>&#34;s3&#34;</span>,
  <span style=color:#f92672>&#34;display_name&#34;</span>: <span style=color:#e6db74>&#34;objcet_storage&#34;</span>,
  <span style=color:#f92672>&#34;email&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
  <span style=color:#f92672>&#34;suspended&#34;</span>: <span style=color:#ae81ff>0</span>,
  <span style=color:#f92672>&#34;max_buckets&#34;</span>: <span style=color:#ae81ff>1000</span>,
  <span style=color:#f92672>&#34;subusers&#34;</span>: [],
  <span style=color:#f92672>&#34;keys&#34;</span>: [
    {
      <span style=color:#f92672>&#34;user&#34;</span>: <span style=color:#e6db74>&#34;x&#34;</span>,
      <span style=color:#f92672>&#34;access_key&#34;</span>: <span style=color:#e6db74>&#34;xxxxxxxxkey01&#34;</span>,
      <span style=color:#f92672>&#34;secret_key&#34;</span>: <span style=color:#e6db74>&#34;xxxxxxxxkey01&#34;</span>
    }
  ],
  <span style=color:#f92672>&#34;swift_keys&#34;</span>: [],
  <span style=color:#f92672>&#34;caps&#34;</span>: [],
  <span style=color:#f92672>&#34;op_mask&#34;</span>: <span style=color:#e6db74>&#34;read, write, delete&#34;</span>,
  <span style=color:#f92672>&#34;system&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,
  <span style=color:#f92672>&#34;default_placement&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
  <span style=color:#f92672>&#34;default_storage_class&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
  <span style=color:#f92672>&#34;placement_tags&#34;</span>: [],
  <span style=color:#f92672>&#34;bucket_quota&#34;</span>: {
    <span style=color:#f92672>&#34;enabled&#34;</span>: <span style=color:#66d9ef>false</span>,
    <span style=color:#f92672>&#34;check_on_raw&#34;</span>: <span style=color:#66d9ef>false</span>,
    <span style=color:#f92672>&#34;max_size&#34;</span>: <span style=color:#ae81ff>-1</span>,
    <span style=color:#f92672>&#34;max_size_kb&#34;</span>: <span style=color:#ae81ff>0</span>,
    <span style=color:#f92672>&#34;max_objects&#34;</span>: <span style=color:#ae81ff>-1</span>
  },
  <span style=color:#f92672>&#34;user_quota&#34;</span>: {
    <span style=color:#f92672>&#34;enabled&#34;</span>: <span style=color:#66d9ef>false</span>,
    <span style=color:#f92672>&#34;check_on_raw&#34;</span>: <span style=color:#66d9ef>false</span>,
    <span style=color:#f92672>&#34;max_size&#34;</span>: <span style=color:#ae81ff>-1</span>,
    <span style=color:#f92672>&#34;max_size_kb&#34;</span>: <span style=color:#ae81ff>0</span>,
    <span style=color:#f92672>&#34;max_objects&#34;</span>: <span style=color:#ae81ff>-1</span>
  },
  <span style=color:#f92672>&#34;temp_url_keys&#34;</span>: [],
  <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;rgw&#34;</span>,
  <span style=color:#f92672>&#34;mfa_ids&#34;</span>: []
}
</code></pre></div><p>从命令行的输出中，可以看到 <code>access_key: xxxxxxxxkey01</code> 和 <code>secret_key: xxxxxxxxkey01</code>，我们将其保存下来，后面还要用。</p>
<p>使用 Ceph 的 RGW 对象存储，可以有很多工具，如 s3cmd、 minio-client 等，这里我们以 s3cmd 为例。有兴趣的还可以尝试
<a href=https://github.com/minio/minio>minio</a>可以可视化进行操作。</p>
<p>如是在 Ubuntu 中，执行 <code>apt install s3cmd</code> 先安装 AWS s3 API，再继续接下来的操作：</p>
<p>我们执行 <code>s3cmd --configure</code> 去进行相关的配置。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>&gt; Access Key: <span style=color:#e6db74>&#34;xxxxxxxxkey01&#34;</span>
&gt; Secret Key: <span style=color:#e6db74>&#34;xxxxxxxxkey02&#34;</span>

&gt; S3 Endpoint <span style=color:#f92672>[</span>s3.amazonaws.com<span style=color:#f92672>]</span>: <span style=color:#e6db74>&#34;&lt;bootstrap_host_ip:80&gt;&#34;</span>，如 <span style=color:#e6db74>&#34;10.1.1.2:80&#34;</span>
DNS-style bucket....<span style=color:#f92672>[</span>%<span style=color:#f92672>(</span>bucket<span style=color:#f92672>)</span>s.s3.amazonaws.com<span style=color:#f92672>]</span>: <span style=color:#e6db74>&#34;&lt;bootstrap_host_ip:80&gt;/%(bucket)s&#34;</span>，如<span style=color:#e6db74>&#34;10.1.1.2:80/%(bucket)s&#34;</span>
HTTPS 选 no，其余基本默认enter
</code></pre></div><p>如果 fail 了，可进入到刚刚保存新建的 config：<code>/root/.s3cfg</code>中，参考如下，修改处末尾用了 <code>#</code> 标识：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#f92672>[</span>default<span style=color:#f92672>]</span>
access_key <span style=color:#f92672>=</span> xxxxxxxxkey01 <span style=color:#75715e>#</span>
...
cloudfront_host <span style=color:#f92672>=</span> 10.1.1.2:80 <span style=color:#75715e>#</span>
...
host_base <span style=color:#f92672>=</span> 10.1.1.2:80 <span style=color:#75715e>#</span>
host_bucket <span style=color:#f92672>=</span> 10.1.1.2:80/%<span style=color:#f92672>(</span>bucket<span style=color:#f92672>)</span>s <span style=color:#75715e>#</span>
...
secret_key <span style=color:#f92672>=</span> xxxxxxxxkey02 <span style=color:#75715e>#</span>
...
</code></pre></div><p>查看 Bucket：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>s3cmd ls
</code></pre></div><p>一开始没有创建过 Bucket，故没有输出，我们来新建一个 <code>s3cmd mb s3://s3cmd-demo</code>，再执行 <code>s3cmd ls</code>，即可看到新创建的 bucket。</p>
<p>
<a href=https://www.cnblogs.com/sunhongleibibi/p/11661123.html>参考资料：使用 s3cmd</a></p>
<p>参考以上资料，可以尝试继续上传文件、上传文件夹、下载、<code>ls</code> 、删除等命令，体验 ceph-rgw 的 Bucket 与 S3 存储的交互。</p>
<blockquote class="book-hint info">
<p>在 Dashboard 里有丰富的信息，可以多多尝试。如查看 rgw 的用户、查看 Bucket、Pool 等，欢迎多多体验。</p>
<p>如上传文件不成功，有提示：<code>ERROR: S3 error: 416 (InvalidRange)</code> 的错误，同样也可释放掉一些先前创建的资源，如 drop 掉 CephFS 再试试，Dashboard 里也能删除 Pool。OSD 只有 3 个，导致 PG 数量吃紧可能不够用。</p>
</blockquote>
<h2 id=ceph-rbd-选>
Ceph RBD (选)
<a class=anchor href=#ceph-rbd-%e9%80%89>#</a>
</h2>
<blockquote class="book-hint info">
<p>
<a href=https://docs.ceph.com/en/pacific/rbd/index.html>参考：CEPH BLOCK DEVICE</a></p>
<p>RBD 即 RADOS Block Device 的简称，RBD 块存储是最稳定且最常用的存储类型。RBD 块设备类似磁盘可以被挂载。RBD 块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在 Ceph 集群的多个 OSD 中。如下是对 Ceph RBD 的理解：</p>
<ul>
<li>RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；</li>
<li>resizable：这个块可大可小；</li>
<li>data striped：这个块在 Ceph 里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；</li>
<li>thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U 盘，存了一个 2G 的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间；</li>
</ul>
<p>块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。</p>
<p>Ceph 可以通过内核模块和 librbd 库提供块设备支持。客户端可以通过内核模块挂在 rbd 使用，客户端使用 rbd 块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过 librbd 使用 ceph 块，典型的是云平台的块存储服务（如下图），云平台可以使用 rbd 作为云的存储后端提供镜像存储、volume 块或者客户的系统引导盘等。</p>
</blockquote>
<ul>
<li>创建 RBD</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph osd pool create rbd <span style=color:#ae81ff>8</span>
<span style=color:#75715e># 值调小些，因为云平台资源有限，3个OSD的PG数默认是有上限的</span>
</code></pre></div><ul>
<li>application enable RBD</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ceph osd pool application enable rbd rbd
</code></pre></div><ul>
<li>创建 rbd 存储, 指定大小为 1GB</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>rbd create rbd1 --size <span style=color:#ae81ff>1024</span>
</code></pre></div><ul>
<li>查看 rbd 信息</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>&gt; rbd --image rbd1 info
rbd image <span style=color:#e6db74>&#39;rbd1&#39;</span>:
	size <span style=color:#ae81ff>1</span> GiB in <span style=color:#ae81ff>256</span> objects
	order <span style=color:#ae81ff>22</span> <span style=color:#f92672>(</span><span style=color:#ae81ff>4</span> MiB objects<span style=color:#f92672>)</span>
	snapshot_count: <span style=color:#ae81ff>0</span>
	id: ace58352cf47
	block_name_prefix: rbd_data.ace58352cf47
	format: <span style=color:#ae81ff>2</span>
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Wed Dec  <span style=color:#ae81ff>1</span> 03:55:11 <span style=color:#ae81ff>2021</span>
	access_timestamp: Wed Dec  <span style=color:#ae81ff>1</span> 03:55:11 <span style=color:#ae81ff>2021</span>
	modify_timestamp: Wed Dec  <span style=color:#ae81ff>1</span> 03:55:11 <span style=color:#ae81ff>2021</span>
</code></pre></div><p>继续执行以下命令：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>&gt; root@ceph:/mnt# ceph osd crush tunables hammer
adjusted tunables profile to hammer

&gt; root@ceph:/mnt# ceph osd crush reweight-all
reweighted crush hierarchy

<span style=color:#75715e># 关闭一些内核默认不支持的特性</span>
&gt; root@ceph:/mnt# rbd feature disable rbd1 exclusive-lock object-map fast-diff deep-flatten

<span style=color:#75715e># 查看特性是否已禁用</span>
&gt; root@ceph:/mnt# rbd --image rbd1 info | grep features
	features: layering
	op_features:

<span style=color:#75715e># 映射到客户端(在需要挂载的客户端运行)</span>
&gt; root@ceph:/mnt# rbd map --image rbd1
/dev/rbd0

<span style=color:#75715e># 查看映射情况</span>
&gt; root@ceph:/mnt# rbd showmapped
id  pool  namespace  image  snap  device
<span style=color:#ae81ff>0</span>   rbd              rbd1   -     /dev/rbd0
</code></pre></div><p>再继续！</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># 格式化磁盘</span>
&gt; root@ceph:/mnt# mkfs.xfs /dev/rbd0
meta-data<span style=color:#f92672>=</span>/dev/rbd0              isize<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>    agcount<span style=color:#f92672>=</span>8, agsize<span style=color:#f92672>=</span><span style=color:#ae81ff>32768</span> blks
         <span style=color:#f92672>=</span>                       sectsz<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>   attr<span style=color:#f92672>=</span>2, projid32bit<span style=color:#f92672>=</span>1
         <span style=color:#f92672>=</span>                       crc<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>        finobt<span style=color:#f92672>=</span>1, sparse<span style=color:#f92672>=</span>1, rmapbt<span style=color:#f92672>=</span>0
         <span style=color:#f92672>=</span>                       reflink<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
data     <span style=color:#f92672>=</span>                       bsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   blocks<span style=color:#f92672>=</span>262144, imaxpct<span style=color:#f92672>=</span>25
         <span style=color:#f92672>=</span>                       sunit<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>     swidth<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span> blks
naming   <span style=color:#f92672>=</span>version <span style=color:#ae81ff>2</span>              bsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   ascii-ci<span style=color:#f92672>=</span>0, ftype<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
log      <span style=color:#f92672>=</span>internal log           bsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   blocks<span style=color:#f92672>=</span>2560, version<span style=color:#f92672>=</span>2
         <span style=color:#f92672>=</span>                       sectsz<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>   sunit<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span> blks, lazy-count<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
realtime <span style=color:#f92672>=</span>none                   extsz<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   blocks<span style=color:#f92672>=</span>0, rtextents<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>

<span style=color:#75715e># 创建挂载目录, 并将 rbd 挂载到指定目录</span>
&gt; root@ceph:/mnt# mkdir /mnt/rbd
&gt; root@ceph:/mnt# mount /dev/rbd0 /mnt/rbd/

<span style=color:#75715e># 查看挂载情况</span>
&gt; root@ceph:/mnt# df -hl | grep rbd
/dev/rbd0      1014M   40M  975M   4% /mnt/rbd
</code></pre></div><blockquote class="book-hint info">
和 CephFS 类似，我们同样可在挂载的目录中创建修改文件，感受 Ceph 的能力——如分布式存储的容错啊，存储共享等。
</blockquote>
<h2 id=实验报告模板>
实验报告模板
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e6%8a%a5%e5%91%8a%e6%a8%a1%e6%9d%bf>#</a>
</h2>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown># Lab03 Ceph 存储实践
<span style=color:#66d9ef>
</span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>班级：
</span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>学号：
</span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>姓名：
</span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>合作者们：[&#39;学号-姓名&#39;, &#39;学号-姓名&#39;]
</span><span style=font-style:italic></span>
---

<span style=color:#75715e>## 基本概念思考
</span><span style=color:#75715e></span>
回答下列问题：

<span style=color:#75715e>### Pool 与 Placement Group (PG) 与 Placement Group for Placement purpose (PGP)
</span><span style=color:#75715e></span>
<span style=color:#75715e>### Ceph 集群的 HEALTH STATUS
</span><span style=color:#75715e></span>
<span style=color:#75715e>## Ceph 部署
</span><span style=color:#75715e></span>
<span style=color:#75715e>&lt;!-- 本部分内容可以根据部署方式的不同进行不同的改动，这里是使用Cephadm部署的模板 --&gt;</span>

<span style=color:#75715e>### 实验前置准备
</span><span style=color:#75715e></span>
<span style=color:#75715e>&lt;!-- 可写内容包括： --&gt;</span>
<span style=color:#75715e>&lt;!-- 云平台/本机，操作系统的具体版本，各台机器的Hostname --&gt;</span>
<span style=color:#75715e>&lt;!-- 一些前置工具的安装、配静态IP、改Hostname的记录 --&gt;</span>

<span style=color:#75715e>### Bootstrap
</span><span style=color:#75715e></span>
<span style=color:#75715e>### 添加 Host
</span><span style=color:#75715e></span>
<span style=color:#75715e>### 创建 OSD
</span><span style=color:#75715e></span>
<span style=color:#75715e>## Ceph Filesystem (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>### 部署 CephFS (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>### 挂载 CephFS (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>## Ceph RGW 对象存储 (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>### Deploy RGW (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>### 使用对象存储 (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>## Ceph RBD (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>## 自行扩展和设计内容 (选)
</span><span style=color:#75715e></span>
<span style=color:#75715e>&lt;!-- 本次实验可自行自由进行扩展，体验和尝试Ceph的各项功能，做得*非常非常非常突出*可直接将其作为申优答辩的内容 --&gt;</span>

<span style=color:#75715e>## 实验总结与心得
</span></code></pre></div></article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
<div><a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/commit/ce882738d5a205b5d1e46acc582195eb7f9cf92d title="Last modified by Yinghao Zhu | December 2, 2021" target=_blank rel=noopener>
<img src=/ns-labs/svg/calendar.svg class=book-icon alt=Calendar>
<span>December 2, 2021</span>
</a>
</div>
<div>
<a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/edit/main/exampleSite/content/docs/table-of-contents/ceph.md target=_blank rel=noopener>
<img src=/ns-labs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span>
</a>
</div>
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
</footer>
<div class=book-comments>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<nav id=TableOfContents>
<ul>
<li><a href=#lab03-ceph-存储集群实践>Lab03 Ceph 存储集群实践</a>
<ul>
<li><a href=#实验目的>实验目的</a></li>
<li><a href=#实验说明>实验说明</a></li>
<li><a href=#概述>概述</a></li>
<li><a href=#重要概念>重要概念</a>
<ul>
<li></li>
</ul>
</li>
<li><a href=#ceph-部署>Ceph 部署</a>
<ul>
<li><a href=#cephadm>Cephadm</a></li>
<li><a href=#部署前操作>部署前操作</a></li>
<li><a href=#部署>部署</a></li>
</ul>
</li>
<li><a href=#ceph-filesystem-选>Ceph Filesystem (选)</a>
<ul>
<li><a href=#部署-cephfs>部署 CephFS</a></li>
<li><a href=#挂载-cephfs>挂载 CephFS</a></li>
</ul>
</li>
<li><a href=#ceph-rgw-对象存储-选>Ceph RGW 对象存储 (选)</a>
<ul>
<li><a href=#deploy-rgw>Deploy RGW</a></li>
<li><a href=#使用对象存储>使用对象存储</a></li>
</ul>
</li>
<li><a href=#ceph-rbd-选>Ceph RBD (选)</a></li>
<li><a href=#实验报告模板>实验报告模板</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</aside>
</main>
</body>
</html>