[{"id":0,"href":"/ns-labs/docs/table-of-contents/raid/","title":"Lab01 RAID 阵列","section":"Table of Contents","content":"Lab01 RAID 阵列 #  实验目的 #  通过 RAID 原理实践理解阵列相关知识\n实验内容 #  本实验通过软件 RAID 管理磁盘：\n 打开“管理工具-计算机管理-存储-磁盘管理”，体验“磁盘管理工具” C 盘扩展卷 检查其它磁盘是否联机，联机并初始化磁盘  尝试创建跨区卷并存储部分数据 尝试创建镜像卷并存储部分数据 尝试创建 RAID5 卷并存储部分数据   通过设备管理禁用其中一块磁盘，模拟磁盘损坏。观察有什么情况发生，数据是否损坏或者丢失 [选做] 自行设计与扩展，体会 RAID 0 1 5 10 等方案下，在遇到磁盘损坏故障、冗余备份的效果等 [选做] 思考题：  为什么及在什么条件下选择用 RAID RAID \u0026amp; 分布式存储 \u0026amp; 集中存储的区别    请在 10 月 11 日中午 12：00 前提交至云平台，命名为：lab01-学号-姓名.pdf，如 lab01-18373722-朱英豪.pdf。\n实验指南 #  通过实验平台获取虚拟机，通过远程桌面访问获取的虚拟机的 IP 地址（ipv4），用户名密码通过实验平台获取。\n按要求完成实验，撰写实验报告（每种配置都做一遍），以下为样例的 IP 和云平台默认分配的用户名、密码。\n登录IP：10.200.200.155 # 请到云平台上查看自己所分配到的虚拟机的 IP 地址 登录用户名：Administrator # 注意在连接虚拟机时需要指定用户名，不是 admin 登录密码：@buaa21 # 记得改密码，防止被别人登 分配磁盘时不宜过大，1 G 足矣，可选择使用“快速格式化”，提高实验速度。\n如何连接虚拟机 #   在 Windows 中可使用“远程桌面连接”工具——win+R 打开“运行”，然后输入 mstsc，然后在弹出窗口中输入 IP、账号、密码即可。 在 MacOS 上可使用 Parallels 等工具连接  注意：\n云平台上没有显示虚拟机 IP 或显示的 IP 无法登录怎么办？首先，可以用来登录的 IP 位于 10.251.252.1~10.251.255.254 之间。\n 如果未登录过实验虚拟机：云平台上显示的 IP 是 169 或者无 IP 则暂时无法登录，需要重启虚拟机重新获取 IP。需要注意的是，虚拟机的重启、以及 DHCP IP 的获取都需要时间，开机后应等待 3 分钟以上，请勿频繁开关机 如果之前登录过实验虚拟机：云平台上显示的是 192 开头的 IP 或者无 IP，可以尝试使用之前的可用 IP 进行登录。如果无法登录（包括密码错误），请联系助教解决。  可能出现“发生身份验证错误，要求的函数不受支持”的错误，请参考该 博客，使用解决方案一：配置本地机的组策略。\n Win+R 在弹出的“运行”框中，输入 gpedit.msc 进入到：计算机配置-\u0026gt;管理模板-\u0026gt;系统-\u0026gt;凭据分配-\u0026gt;加密数据库修正-\u0026gt;启动该策略，将保护层级设置为“易受攻击\u0026quot;。  重要提醒：\n由于分配的 IP 均为 DHCP 自动获取，有可能实验虚拟机的 IP 会发生变化，尤其是在长时间关机再开启后。因此请登录实验虚拟机后，及时更改虚拟机密码，以避免以下情况发生：\n 未意识到登录了他人的虚拟机， 帮助别人完成了实验工作 自己的虚拟机被别人登录，已经做完的实验被破坏  同时，虚拟机密码请设置非常用敏感密码。因为助教在帮助大家解决问题时往往需要你的密码登录到你的虚拟机。\n自行配置环境 #  你也可以自己在 VMware 等虚拟机中安装 Windows Server 2019 (或其他版本)，不使用云平台的资源去做实验。可至 Next, ITELLYOU 该网站上去下载相关系统的 ISO 镜像，本课程网站上的“Resources”区，给出了其 ED2K 的下载链接以及 KMS 激活方法。\n参考作业提交的 Markdown 模板 #  # Lab01 RAID 阵列 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名：  ## 实验内容  ### 体验“磁盘管理工具”  ### C 盘扩展卷  ### 检查其它磁盘是否联机，联机并初始化磁盘  - 尝试创建跨区卷并存储部分数据 - 尝试创建镜像卷并存储部分数据 - 尝试创建 RAID5 卷并存储部分数据 ### 通过设备管理禁用其中一块磁盘，模拟磁盘损坏。 \u0026gt; 观察有什么情况发生，数据是否损坏或者丢失  ## 实验感想 "},{"id":1,"href":"/ns-labs/docs/resources/os/","title":"操作系统","section":"Resources","content":"操作系统 #  常见操作系统 ISO 镜像可至官网、 MSDN，我告诉你、 Next, ITELLYOU 等网站上去下载。\nWindows Server 2019 #  ED2K 下载：ed2k://|file|cn_windows_server_2019_updated_july_2020_x64_dvd_2c9b67da.iso|5675251712|19AE348F0D321785007D95B7D2FAF320|/\n使用 KMS 激活方式如下：\n# 使用管理员身份打开PowerShell DISM /online /Set-Edition:ServerDatacenter /ProductKey:WMDGN-G9PQG-XVVXX-R3X43-63DFG /AcceptEula # 打开 CMD (管理员) slmgr.vbs /ipk WMDGN-G9PQG-XVVXX-R3X43-63DFG slmgr.vbs /skms kms.teevee.asia slmgr.vbs /ato "},{"id":2,"href":"/ns-labs/docs/table-of-contents/virtual/","title":"Lab02 虚拟化实验","section":"Table of Contents","content":"Lab02 虚拟化实验 #  实验目标 #   实施计算虚拟化，安装配置环境，熟悉计算虚拟化的概念，理解基本操作，掌握基础知识。 理解集中管理对于虚化的作用，通过部署集中vCenter体验集群的设置，分布式交换机的设置，了解主机从不同网络进行迁移的实际需求。  实验内容 #   按照实验指南的指导，完成实验。 按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。  实验可以在分配的虚拟机中进行，也可以在本地机器中进行。（如果本地机器内存大于12G，则建议在本地进行，以获得更流畅的实验体验）。在本地进行实验的同学，可以前往 这里下载需要的资源；使用云平台虚拟机的同学，相应的资源已经放到了虚拟机桌面上。\n请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab02-学号-姓名.pdf，如 lab02-18373722-朱英豪.pdf。\n前情提要 #  推荐开始试验前，首先查看 资源文件夹中的”虚拟化实验.pptx“。\n有任何问题，请首先阅读 FAQ\n如果还有问题，还可尝试阅读 官方文档（如下图所示，请注意选择正确的版本号）\n  Hypervisor #  我们知道，一台计算机一般有以下的结构：\n操作系统负责管理硬件资源（CPU，内存，硬盘等），并向上提供相应的系统调用，供具体的应用程序使用。\n而我们平常提到的操作系统的虚拟化，本质上就是要模拟出一套硬件（包括虚拟CPU，虚拟内存，虚拟硬盘等），然后在这一套虚拟的硬件的基础上部署客户操作系统。客户操作系统完全不需要做任何修改，即可在这个“虚拟的机器”中顺利执行。但客户操作系统的运行结果（比如接收键盘输入，输出图像和声音等），最终都是要靠原始的“实实在在”的硬件（物理机）来完成的。也就是说，需要有那么一个结构，能够将这个“虚拟的机器”的行为翻译到物理机的行为（比如将虚拟CPU的指令翻译到物理机的CPU指令）。负责做这件事情的结构被称为Hypervisor，又称为虚拟机监控器（virtual machine monitor，缩写为 VMM）。\n根据工作方式的不同，Hypervisor分为以下两种。\n 第一种是我们比较熟悉的情况，本质上就是在主操作系统（Host Operating System）上安装了一个虚拟化软件，它来负责充当虚拟机的管理者，并通过主操作系统的系统调用来完成对物理机硬件的使用。VMware Workstation、Virtual Box、Qemu等都属这类虚拟化软件。除了这个虚拟化软件之外，主操作系统上还会运行其他“正常”的应用程序，比如，你在用VMware Workstation的同时还能听歌聊天等。\n第二种Hypervisor则直接舍弃了主操作系统（因为毕竟隔着一层，性能会有损失），而是直接把Hypervisor部署在硬件上。在这种情况下，物理机变成了更纯粹的“为虚拟化而生”的机器。Hypervisor能够直接与硬件沟通，其实在某种程度上也承担了主操作系统的角色（管理硬件），因此，我们也可以把这种Hypervisor看作是一种为虚拟化特制的操作系统。这其中典型的就是VMware ESXi。\n因为我们不可能要求每位同学都制备一套硬件来安装学习VMware ESXi，所以需要首先使用VMware Workstation来模拟出一套硬件。但VMware Workstation仅仅起一个前置作用，在实际的实验中并不会涉及到。请大家首先理清这层关系。\n实验指南 #  0. 安装VMware Workstation #  使用分配的虚拟机的桌面上的安装包安装即可。\n安装完成后需重启机器。\n打开VMware Workstation时， 选择试用即可。\n 1. 安装VMware ESXi #  使用桌面上的ESXi镜像创建虚拟机。\n注意选择客户操作系统的类型。   虚拟机创建完成后，直接打开电源即可启动ESXi操作系统的安装流程，这一过程可能需要等待较长时间。\n安装流程中总是保持默认选项即可。\n在此流程中，可能需要使用使用到某些快捷键，这些快捷键可能会首先被你本机的操作系统捕获，在本机的系统设置中暂时屏蔽该快捷键即可。  请务必牢记设置的root密码。   安装完成后，可以看到如下界面。\n 可以看到，ESXi系统获得了一个IPv4地址192.168.80.128，并且这个地址是通过DHCP的方式获得的。这里用到的DHCP服务器其实是VMware Workstation内置的。也就是说，192.168.80.128这个地址只有在安装VMware Workstation的机器上才是有效的。\n2. 访问ESXi #  一般情况下，可以使用桌面客户端VMware vSphere Client作为“Client”和ESXi（作为“Server”）通信。\nVMware vSphere Client使用如下图所示的安装包安装即可。\n 安装完成后，打开vSphere Client，填入ESXi的IP地址、root的用户名和密码后即可建立起与ESXi的连接。\n如果弹出安全证书警告，直接忽略即可。   除了桌面客户端，也可以直接使用浏览器访问。访问的地址就是ESXi的地址，用户名和密码与vSphere Client的相同。\n 如下图所示，可以为ESXi分配许可证。\n可用的KEY：\n4F092-DML5N-H8E20-4HC54-17K4D 1G2J8-4K097-48DU1-HH3G2-C6H60 4V4JR-40317-H88H1-83ANP-23H76 1G002-DRK5J-08429-K035K-0KK16 NY01K-AP343-M89H8-WJ9EK-2VK1F 1F29K-4W14M-H8099-PA8X2-9KUHD  后续实验内容可以在Web端进行，也可以在vSphere Client桌面客户端进行。\n3. 观察和体验vSphere Client提供的功能 #  Client侧界面主要包含导航器、主体内容和任务事件这三部分。\n 请浏览左侧导航栏的不同模块和不同模块下不同选项卡的内容，对ESXi提供的功能有个大致的了解。\n其中，存储部分可以查看ESXi虚拟机可访问的数据内容。\n 可以通过使用“数据存储浏览器”查看、下载、上传、下载“存储”中的文件。\n4. 资源池 #  资源池可以实现对CPU和内存的池化管理。\n尝试在vSphere Client中新建资源池。\n 理解资源池“预留”和“限制”的含义。\n 5. 新建和安装虚拟机 #  ESXi最主要的功能就是对虚拟机的管理。\nvSphere Client和Web端都有显著的入口供用户创建虚拟机。\n  创建虚拟机的流程与使用VMware Workstation创建虚拟机没有太大区别，按照创建向导进行即可。\n在创建过程的“自定义设置”阶段，需要手动配置CD/DVD驱动器，插入桌面上的CentOS或Tiny Core Linux的安装镜像，以使虚拟机在开机时，能自动进入安装镜像的安装引导界面。\n 当然，你也可以在虚拟机创建完成后，打开电源之前，手动编辑虚拟机配置，添加对应的镜像文件。\n 完成后，打开虚拟机电源，即可进入操作系统的安装引导流程。\n6. 虚拟机文件系统格式和种类 #  添加虚拟机后，可以在“存储”中看到每台虚拟机中包含的文件内容。\n 了解这些不同格式的文件和含义和作用。\n7. 虚拟机导出 #  关闭虚拟机电源后，即可将虚拟机导出。\n 观察导出文件格式与虚拟机正常文件的区别。\n8. VMware Tools（选做） #  可以在如下图所示的虚拟机控制台的“操作”选项中找到“安装VMware Tools”的选项。\n 点击“安装VMware Tools”后，ESXi会给虚拟机挂载一个包含了安装脚本的光盘。根据安装的操作系统的不同，具体的安装方式也有区别。对于Linux系统，可以在/dev目录下找到该光盘，并将其挂载到文件系统中，然后进入其中，执行安装脚本。\n9. 角色和用户（选做） #  可以在Web端创建不同的角色和用户，根据角色的不同，用户将获得不同的权限。尝试使用不同的用户账号登录Client，以体验权限限制带来的差异。\n 10. 创建与配置集群（选做） #  后面这两部分实验内容比较复杂，实验环境也不是特别理想，有兴趣的同学可以选择尝试。\n在上面的实验中，我们进行了ESXi的部署，并使用ESXi创建了一些虚拟机。在实际应用中，往往需要多个ESXi主机组成集群，来提供更多的资源，或者提高可用性。在接下来的实验中，我们将使用vCenter Server管理多个ESXi主机，来管理所有的虚拟机和ESXi“物理机”集群。\n  在VMware Workstation中新建一个新的ESXi虚拟机。该虚拟机内存可以给大一些，比如14G，磁盘大小也可以大一些，比如100G。以方便后续在其上安装vCenter Server。\n可以在VMware Workstation中给机器开大内存和大容量磁盘。\n因为内存和磁盘是虚拟的，所以即使本机物理内存和磁盘容量不够，也把握得住。\n如果后续实验中，确实出现了内存或磁盘容量不够的情况，请联系助教扩容。\n   安装vCenter Server\n  VMware-VIM-all-6.5.0-4602587.iso   打开vCenter Server安装包 VMware-VIM-all-6.5.0-4602587.iso，双击autorun启动安装程序。之后的安装步骤，如无特殊说明，无需改动默认选项，直接下一步即可   点击[安装]按钮，（等待几十秒），会弹出另一个安装页面。选择默认选项“vCenter Server 和嵌入式Platform Services Controller”。\n  在系统网络名称页面，要填入虚拟机的本机IP。点击确认后会弹出两次提示，确认并无视即可。本实验中应该是10.251开头的IP   在vCenter Single Sing-On 配置页面，需要设置登录vCenter的密码。域名建议不改动，密码需要符合下面的要求，站点名称可以随意更改。   vCenter Server所需的端口。如果点击下一步时提示某个端口被占用，在cmd中执行netstat -ano | findstr \u0026quot;{port_num}\u0026quot;即可找到占用端口的进程（把{port_num}替换为相应的端口号，注意两侧有引号），找到对应的进程号后，使用tskill ${pid}kill对应的进程即可。其中，80端口的进程号为4的服务可以使用NET stop HTTP杀掉。不推荐更改默认端口号。   安装完成后，浏览器访问虚拟机地址或者使用vSphere Client均可访问到vCenter。账号为administrator@vsphere.local，密码为第4步中设置的密码。\n  创建数据中心。\n  添加主机。把本次实验创建的两个ESXi主机都添加进去。在添加主机时会弹出安全警示，选择“是”继续添加即可。   VMware-VCSA-all-6.5.0-4602587.iso   安装打开vCenter Server安装包 VMware-VCSA-all-6.5.0-4602587.iso，在本机（或虚拟机）中装载该ISO文件，可以看到README文件，其中包含着具体的安装指引。之后的安装步骤，如无特殊说明，无需改动默认选项，直接下一步即可   我们刚刚启动的只是一个安装器程序，vCenter Server本质上还是要安装在一个特定的ESXi主机中。所以，在选择安装目标时，需要填入刚才新建的那个内存和磁盘容量都比较大的ESXi虚拟机的相关信息。   在最后的网络配置这里，选择DHCP   安装完成后，浏览器访问虚拟机地址或者使用vSphere Client均可访问到vCenter。\n  创建数据中心。\n  添加主机。把本次实验创建的两个ESXi主机都添加进去。在添加主机时会弹出安全警示，选择“是”继续添加即可。    11. 分布式交换机（选做） #  默认情况下，两台主机中的虚拟机之间的网络是不通的。这很显然不符合我们的需求——搭建集群的主要目的就是为了整合不同主机的资源，虚拟机对其在哪个主机上应该是无感的。\n因此，就需要使用分布式交换机，将两个或多个主机的网络连通起来。\n感兴趣的同学，可以查找相关资料，在配置好的集群上，创建分布式虚拟交换机。并分别在不同的主机上部署虚拟机，实现它们之间的网络互通。\n实验报告模板 #  # Lab02 虚拟化实验 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名：  --- ## 实验内容  ### 1. 安装VMware ESXi  ### 2. 访问ESXi  #### ESXi分配许可证  ### 3. 观察和体验vSphere Client提供的功能  ### 4. 资源池  #### “预留”和“限制”  ### 5. 新建和安装虚拟机  ### 6. 虚拟机文件系统格式和种类  #### 不同格式的文件和含义和作用  ### 7. 虚拟机导出  #### 导出文件格式与虚拟机正常文件的区别  ### 8. VMware Tools（选做）  ### 9. 角色和用户（选做）  ### 10. 创建与配置集群（选做）  ### 11. 分布式虚拟交换机（选做）  ## 实验总结与心得  "},{"id":3,"href":"/ns-labs/docs/table-of-contents/ceph/","title":"Lab03 Ceph存储集群实践","section":"Table of Contents","content":"Lab03 Ceph 存储集群实践 #  实验目的 #    了解 Ceph 存储的基本工作原理\n  建立对分布式存储的初步认识\n  实验说明 #  本次实验需要使用至少三台虚拟机，而每位同学只有一台机器。因此可以三名同学选择合作实验。这三名同学的实验报告内容（除“实验总结与心得”外）可以全部相同。请在实验报告中写明提交者是谁，另外的合作者是谁。\n当然，你也可以选择自己完成实验。但这要求你在本地的虚拟机管理工具中，新建 3 台以上的机器进行实验（或者使用其他公有云产品中的机器）。\n对于使用自己的机器的同学，建议采用 CentOS 系统（这样 Kernel 版本不至于太新，减少出错的可能），每台机器配置为 2 核 2G 即可，并且每台机器至少附带一个全新的大于 5G 的虚拟磁盘。\n请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf，如 lab03-18373722-朱英豪.pdf。\nCeph 是一个分布式的存储集群。大多数生产可用的分布式集群的部署安装都不简单，Ceph 也不例外。很多时候，需要对 Linux Kernel 和 Linux 网络管理有基本的了解。\n实验中遇到的困难请及时在课程微信群中抛出。\n实验文档不可能将整个 Ceph 的文档都搬过来，因此，在实验的整个过程中，请务必将 Ceph 官方文档也作为重要的参考依据。实验文档中有描述不到位的地方，可以参考 Ceph 官方文档的相关讨论。\n另外，因为 Ceph 是 Red Hat 主导开发的产品，因此，Red Hat 也有关于 Ceph 完整的技术介绍， 它的文档同样值得参考，比如，你在这里可以找到它的 Ceph 部署指南，甚至在很多方面比 Ceph 官方更加详实。\n 在中文互联网上，你很容易找到 Ceph 的中文文档。但它有很多过时的内容，比如它还在使用ceph-deploy这个官方已经不再维护的工具（这个工具同样不支持新版的 Ceph）来部署集群。因此，它的内容充其量也只能作为部分参考。\n除了中文文档，你还会找到很多良莠不齐的 Blog，这些文章很多也是过时的，在使用时同样务必加以甄别。\n在实验过程中，在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果。当然，勇敢试错是值得鼓励的，但请做好从头开始的准备。\n 概述 #  Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？\n试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。\n最初，你将网站部署在一个装有 500G 硬盘的服务器上。随着时间的流逝，500G 的硬盘逐渐被填满。现在你有两种选择。\n  纵向拓展。在服务器上加装硬盘，甚至你可以使用 LVM 将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。\n  横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。\n  第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。\n但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：\n  如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？\n  如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。\n  随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是 1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。\n  ……\n  上面这些问题，正是 Ceph 这类分布式存储系统所要解决的问题。简单来说，Ceph 是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供分布式、横向拓展、高度可靠性的存储系统。\n对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下 MIT 6.824的课程，并尽量完成它的全部实验。\n在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B 站上有 翻译好的熟肉。\n 除此之外，Ceph 的独特之处还在于，它在一个存储系统上，对外提供了三种类型的访问接口：\n  文件存储。简单来说，你可以将 Ceph 的存储池抽象为一个文件系统，并挂载到某个目录上，然后像读写本地文件一样，在这个新的目录上创建、读写、删除文件。并且该文件系统可以同时被多台机器同时挂载，并被同时读写。从而实现多台机器间的存储共享。\n  对象存储。Ceph 提供了对象存储网关，并同时提供了 S3 和 Swift 风格的 API 接口。你可以使用这些接口上传和下载文件。\n  块存储。Ceph 还能提供块存储的抽象。即客户端（集群外的机器）通过块存储接口访问的“所有数据按照固定的大小分块，每一块赋予一个用于寻址的编号。”客户端可以像使用硬盘这种块设备一样，使用这些块存储的接口进行数据的读写。（一般这种块设备的读写都是由操作系统代劳的。操作系统会对块设备进行分区等操作，并在其上部署文件系统，应用程序和用户看到直接看到的是文件系统的接口（也就是文件存储））。\n   需要注意的是，虽然 Ceph 对外提供了上面这三种不同类型的存储接口，但其底层会使用相同的逻辑对接收的数据进行分块和存储。\n重要概念 #  一个 Ceph 集群必须包含三种类型的进程：Monitor、OSD 和 Manager。其中，Monitor 和 OSD 是最核心的两类进程。\nMonitor 和 OSD #  Monitor 进程负责维护整个系统的状态信息，这些状态信息包括当前的 Ceph 集群的拓扑结构等，这些信息对 Ceph 集群中各个进程的通信来说非常关键。除此之外，Monitor 进程还负责充当外界（也就是官方文档中总是提到的“Client”）与 OSD 进程交流的媒介。\nOSD 进程则负责进行真正的数据存储。如下图所示，外界传送给 Ceph 集群的数据（不管是通过文件存储、对象存储还是块存储的接口）都将被转化为一个个对象（object）。这些 object 将经由 OSD 进程存储到磁盘中。\n 简单来说，当一个 Client 试图向 Ceph 集群读写数据时，将发生以下步骤：\n  Client 向 Monitor 进程请求一个 token 校验信息\n  Monitor 生成 token 校验信息，并将其返回给 Client\n  Monitor 同时会将 token 校验信息同步 OSD 进程\n  Client 携带着 Monitor 返回的 token 校验信息向对应的 token 发送数据读写请求\n  OSD 进程将数据存储到合适的位置，或从合适的位置读出数据\n  OSD 进程向 Client 返回数据\n  以上读写数据的流程是经过极致简化的，主要是为了帮助大家建立对 Monitor 进程和 OSD 进程所起的作用的感性认识。\n想要了解详情，请阅读 Ceph 的文档 - Architecture。\n Manager #  Manager 进程主要负责跟踪当前集群的运行时状况，包括当前集群的存储利用率、存储性能等等。同时，它还负责提供 Ceph Dashboard、RESTful 接口的外部服务。\n需要注意的是，Ceph 集群中有三类这样的进程，但不是每个进程只有一个。\n我们之前提到过，Ceph 是一个有很高容错性的分布式系统，而达到高容错性的一个很重要的方式就是“冗余”。\n比如，对于 Monitor 进程来讲，集群中仅有一个就够用了。但如果运行这一个 Monitor 进程的机器挂了，那么整个集群就会瘫痪（Client 将不知道该跟谁通信来拿到校验信息和集群状态信息等）。因此，一个高可用的 Ceph 集群中会包含多个执行几乎相同任务的运行在不同机器上的 Monitor 进程；这样挂了一个，Client 还可以跟剩下的通信，整个集群依旧可以正常对外提供服务。同样的道理，OSD 进程和 Manager 进程也有多个副本。\n另外，Ceph 为了保证数据的可靠性（也就是说 Client 存储进来的数据不能丢失）——注意区分其与整个系统可靠性的区别——在默认情况下，会将每份数据存储3 份，每份都会存储在不同的 OSD 上（鸡蛋不能放到同一个篮子里）。这样，即使有部分 OSD 挂掉，也能保证大部分数据不会丢失。因此，一个健康的 Ceph 集群要求至少同时存在三个健康的 OSD 进程（当然，这个默认的数值可以更改）。\n Pool 与 Placement Group (PG) 与 Placement Group for Placement purpose (PGP) #  请查阅 Ceph 的相关文档，阐述 Pool、PG、PGP 与 OSD 之间的关系。可定性/定量分析 OSD、PG、PGP、OSD、PG_NUM 之间的数量关系。\nCeph 集群的 HEALTH STATUS #  你在实验过程中，打 ceph -s 都遇到过哪些 HEALTH STATUS？（ceph health detail 能看到更为详细的状态信息）如果是 WARN/ERROR 都是哪些原因？\nPG_NUM 的相关计算 #  请查阅 Ceph 的相关文档，\nCeph 部署 #  本节内容的目标是创建一个可用的 Ceph 集群。其中包括，至少一个 Monitor 进程、至少一个 Manager 进程、至少三个 OSD 进程。\n使用云平台提供的机器进行实验的同学，请直接使用 root 用户登录，密码是\u0026amp;shieshuyuan21。\n登录后，请首先使用passwd命令修改密码。\n Ceph 官方提供了 多种部署方式。\n在本实验文档中，我们采用 Cephadm作为部署工具。Cephadm 也是官方推荐的部署和管理 Ceph 集群的工具，它不仅可以用来部署 Ceph，还可以在安装完成后，用来管理集群（添加和移除节点、开启 rgw 等）。\n当然，如果你对其他部署方式感兴趣的话，也可以尝试；比如，对 Kubernetes 比较感兴趣的同学，可以尝试使用 Rook进行部署。\nCephadm #  Cephadm 是基于“容器技术（Container）”进行工作的，每个 Ceph 的工作进程都运行在相互隔离的容器中。Cephadm 支持使用 Docker 和 Podman 作为容器运行时。在部署时，Cephadm 将首先检查本机中安装的容器运行时类型，当 Docker 与 Podman 并存时，将首先使用 Podman（毕竟 Podman 也是 Red Hat 的产品）。\n在部署时，Cephadm 会首先在本地启动一个 mini 的 Ceph 集群，其中包括 Ceph 集群最基本的 Monitor 进程和 Manager 进程（当然，这两个进程都是通过容器形式运行起来的）。这个 mini 集群在某种程度上来说也是合法的，只不过其基本不能对外提供任何功能。随后，我们将继续使用 Cephadm 提供的工具，将其他机器（后文中也会称之为“节点”）加入到集群中（也就是在其他节点中启动 Ceph 的 Monitor、Manager、OSD 等进程），从而构建一个完整可用的集群。\n下面是 Red Hat 给出使用 Cephadm 构建的集群的架构图。\n 图中的“Container”指的就是我们上面所说的“容器”。\n图中最左边的 Bootstrap Host 就是我们执行 Cephadm 相关命令的机器（事实上，在整个集群的构建过程中，除了修改 IP 和联网等操作外，我们都只会在这个 Bootstrap Host 上进行操作）。我们在 Bootstrap Host 执行的针对其他节点的操作，都是 Cephadm 通过 ssh 的方式发送给对应节点的。\n这个图目前大家心里有个印象就好，在后续的实验操作中，可以反复回来对照检查。\n部署前操作 #  联网 #  部署过程中的操作尽量都联网进行。云平台的 Linux 机器联网操作请参考：\n 推荐使用 Dr-Bluemond/srun提供的工具。云平台提供已经编译好的 Linux64 位版本。可以这样获取：\nwget https://scs.buaa.edu.cn/scsos/tools/linux/buaalogin chmod +x ./buaalogin 使用前请使用config命令配置一下校园网用户名和密码（注意，如果用户名中有英文的话，请大小写都尝试一下）：\n./buaalogin config 配置完成后，使用login命令登录即可：\n./buaalogin login 或，直接：\n./buaalogin 如果想作为系统命令使用的话（注意替换合适的安装路径）：\nsudo install ./buaalogin /usr/local/bin  安装 Cephadm #  对于云平台提供的虚拟机，Cephadm 相关的工具已经安装好，本步骤可以略过。  本步骤需要在各台机器上都要完成  Cephadm 也是个命令行工具，在使用前也需要额外安装这个工具。\n  保证系统已经安装好 curl、Python3、Podman/Docker(推荐使用 Podman，都是自家 RedHat 的产品)\n  下载 Cephadm：\ncurl --silent --remote-name --location https://scs.buaa.edu.cn/scsos/ceph/cephadm 赋予可执行权限；\nchmod +x ./cephadm   安装相关工具（请务必保持联网状态，可能会耗费比较久的时间，请耐心等待）：\n./cephadm add-repo --release octopus ./cephadm install cephadm install ceph-common   配置静态 IP，修改 Hostname #  本步骤需要在各台机器上都要完成  Ceph 集群的各个机器之间是通过 IP 地址进行通信的，而云平台的机器在重启后通过 DHCP 获取的 IP 可能会发生变化，这将给实验带来一定的干扰。\n因此，建议在正式开始实验前，为机器配置静态 IP（即，将你此时的虚拟机的 IP 固定下来，即禁用 DHCP，改为手动指定 IP、网关、DNS 服务器等）。对于使用个人机器进行实验的同学，同样也建议配置一下静态 IP。\n使用云平台提供的机器的同学，请参考下方给出的 CentOS 机器的配置方法。  CentOS 对于 CentOS 的机器，配置 IP 可以手动编辑/etc/sysconfig/network-scripts/下的配置文件。\n也可以采用 NetworkManager 提供的下面这种图形化的终端配置方法。\n登录到虚拟机后，使用nmtui编辑需要配置静态 IP 的网卡：\nnmtui edit ens150 指定正确的网卡 #  上面这条命令中的ens150指的是你要指定静态 IP 的网卡名称。请根据自己的实际情况指定网卡名称。\n如何查看自己的网卡名称？\n在终端中输入ip a，将得到大致如下的输出：\n 可以看到，这里列出了当前机器上的所有网卡及其上的 IP 等信息，并且每个都有编号。\n找到那个标有你的 IP 地址的网卡信息，即可找到它的网卡网卡名称。\n比如，在上图中，你要固定的 IP 是10.251.253.10，找到它对应的那一组网卡信息，即可看到网卡名称是ens160。\n 在打开的如下图所示的界面中，编辑IPv4 CONFIGURATION下面的几项。\n 首先，将Automatic切换为Manual。意思是，不希望通过 DHCP 自动获取 IP，而是手动指定 IP。\n然后，编辑Address，填入你当前的 IP 地址，并给出子网掩码的位数。对于云平台的机器子网掩码位数是22；使用本地机器的同学，请根据实际情况配置。\n然后，编辑Gateway，即网关地址。对于云平台的机器，这一地址是10.251.252.1；使用本地机器的同学，请根据实际情况配置。\n然后，编辑DNS servers，即 DNS 服务器地址。对于校园网内的机器，都可以将 DNS 地址指定为202.112.128.50。\n之后，选择最下面的OK退出即可。\n总而言之，如果使用云平台的机器，最终配置的结果与下图所示应该一直（仅 IP 地址不同）。\n Debian 及 Ubuntu 16.04 直接编辑 /etc/network/interfaces 文件即可。具体可以参考 这篇文章。\n其中，对于云平台的机器，需要指定gateway为10.251.253.10，netmask为255.255.252.0，dns-nameservers为202.112.128.50。\nUbuntu 18.04 及以上 Ubuntu 18.04 开始，Ubuntu 开始使用 Netplan 来管理网络连接。因此，配置静态 IP 需要编辑 /etc/netplan/ 下的配置文件。具体可以参考 这篇文章。\n其中，对于云平台的机器，需要指定gateway为10.251.253.10，netmask为255.255.252.0，nameservers为202.112.128.50。\n 对于使用云平台的机器的同学，配置静态 IP 时，请务必务必保证使用现有的 IP。\n也就是说，配置静态 IP 前后，你的机器的 IP 应该始终相同。\n 配置完静态 IP 后，还需要修改一下机器的 hostname，以区分集群内的各台机器。\necho \u0026#39;你的机器的Hostname\u0026#39; \u0026gt; /etc/hostname 机器的 hostname 其实可以任意指定，但最好有意义，并携带上你的学号，以防止云平台上几十台实验虚拟机的 hostname 发生冲突。\n比如，如果你的学号是15213304，可以将第一台机器的 hostname 指定为ceph-01-15213304，第二台机器的 hostname 指定为ceph-02-15213304。\n配置完静态 IP 和 hostname 后，请使用 reboot 命令重启机器。\n部署 #  基于 Cephadm 的便捷性，部署部分的操作只需要在选定的一台机器（我们把这台机器称为 Bootstrap Host 上执行即可。  在进行部署操作前，请务必保证所有机器处于联网状态。  BootStrap #  我们首先需要在一台选定的机器上，使用cephadm启动一个 mini 集群。\ncephadm --image harbor.scs.buaa.edu.cn/ceph/ceph:v16 bootstrap --mon-ip *\u0026lt;mon-ip\u0026gt;* 请将*\u0026lt;mon-ip\u0026gt;*替换为你执行这命令的机器的 IP。\n如：\ncephadm --image harbor.scs.buaa.edu.cn/ceph/ceph:v16 bootstrap --mon-ip 10.251.219.248 上面这条命令中，--image制定了 Cephadm 启动容器时使用的镜像名称，--mon-ip指定了 Cephadm 要在哪个机器上启动一个 mini 集群。\n更详细地，这条命令将会做如下事情：\n   Create a monitor and manager daemon for the new cluster on the local host.\n  Generate a new SSH key for the Ceph cluster and add it to the root user\u0026rsquo;s /root/.ssh/authorized_keys file.\n  Write a copy of the public key to /etc/ceph/ceph.pub.\n  Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster.\n  Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring.\n  Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring.\n   命令执行完成后，我们可以通过ceph -s查看当前集群的状态。\n 可以看到，确实启动了一个 Monitor 进程和一个 Manager 进程。\n另外，我们还注意到，当前集群的健康状态是HEALTH_WARN，原因下面也列出来了：OSD count 0 \u0026lt; osd_pool_default_size 3。这是因为当前 Ceph 集群默认的每个 Pool 的副本数应该是 3（即，Ceph 中存储的每份数据必须复制 3 份，放在 3 个不同的 OSD 中），但我们 OSD 的进程数是 0。不用着急，马上我们就会创建足够的 OSD 进程（Bootstrap Host、另外两台主机都为他们创建 OSD 进程）。\n mini 集群启动完成后，可以使用podman ps查看当前启动的容器的状态：\n 可以看到，最上面那两个容器，对应的就是 ceph 集群的 Monitor 进程和 Manager 进程。\nCeph Dashboard（可选） #  注意看bootstrap指令的输出，你可以看到一段这样的内容：\n 显然，这是在告诉我们，cephadm 同样启动了一个Ceph Dashboard，这是一个 Ceph 的管理前端。通过访问这个页面，我们就可以以可视化的方式观察到当前集群的状态。\n这段信息中的ceph-01代指的就是你当前这台机器的 IP（当然，如果你的 Hostname 不是ceph-01的话，得到的将是其他的结果）。用你的机器的 IP 替换这个ceph-01，尝试在浏览器访问它。\n是不是访问失败了？这是因为 Ceph Dashboard 默认使用的是 HTTPS 协议，而且它用的 HTTPS 证书还是自己签发的。。。为了能正常访问，我们可以手动禁用 SSL：\nceph config set mgr mgr/dashboard/ssl false 但禁用 SSL 后，Dashboard 服务将默认监听 8080 端口。但在 CentOS 中，8080 端口默认是被防火墙屏蔽的。\n你可以选择手动打开防火墙的 8080 端口；也可以像下面这样，将 Dashboard 服务的监听端口手动改为 8443（因为这个端口就是使用 HTTPS 时 Dashboard 的监听端口，在刚才的 Bootstrap 时已经在防火墙中打开了）：\nceph config set mgr mgr/dashboard/server_port 8443 然后，重启该 Dashboard 服务，使刚才的配置生效。\nceph mgr module disable dashboard ceph mgr module enable dashboard 然后，查看当前的服务状态（如果输出为空的话，耐心多等一会儿）：\nceph mgr services  现在，你应该可以正常访问 Dashboard 服务了。注意，用户名和密码是我们前面提到的 Bootstrap 命令输出的那堆信息中提到的。\n 添加其他节点 #  添加其他节点前，请保证所有节点都处于联网状态  接下来，我们将把其他机器添加到现有的这个 mini 集群中来。\n前面提到过，cephadm 是通过 ssh 协议与其他机器通信的。所以，这里需要首先把 Bootstrap Host 机器的公钥 copy 到其他的所有机器：\nssh-copy-id -f -i /etc/ceph/ceph.pub root@*\u0026lt;new-host\u0026gt;* 例如，如果你的有一台机器的 IP 是10.252.252.40，那么这条命令应该是：\nssh-copy-id -f -i /etc/ceph/ceph.pub root@10.252.252.40 处理完所有的机器后，就可以正式将它们加入到 mini 集群中来了：\nceph orch host add *\u0026lt;newhost\u0026gt;* [*\u0026lt;ip\u0026gt;*] [*\u0026lt;label1\u0026gt; ...*] 例如，你有一台机器的 Hostname 是ceph-02，IP 是10.252.252.40，那么这条命令应该是：\nceph orch host add ceph-02 10.252.252.40 添加完成后，你可以通过ceph -s查看当前集群状态的变化。也可以通过 Ceph Dashboard 看到变化。\n创建 OSD 进程 #  我们知道，OSD 进程是真正用来做数据读写的进程。我们可以用一块专门的磁盘交给 OSD 进程来读写数据，ceph 集群所存储的数据就将保存在这些磁盘中。\n这些被用来交给 OSD 进程管理的磁盘，应该满足以下条件：\n   The device must have no partitions.\n  The device must not have any LVM state.\n  The device must not be mounted.\n  The device must not contain a file system.\n  The device must not contain a Ceph BlueStore OSD.\n  The device must be larger than 5 GB.\n   简单来说，就是将一块干净的磁盘插入机器后，什么都不用做就好。\n在云平台分配的虚拟机中，每台机器都额外插入了一块这样干净的磁盘。如是在自己本机上做，可在 VMware 中添加磁盘，重启后即可看到新添加的干净磁盘 sdb。\n可以通过fdisk -l来查看：\n 注意看上面这两块磁盘：\n  第一个名称是/dev/sda，容量是 16G，有两个分区：/dev/sda1，/dev/sda2。这就是我们现在在使用的这个系统所用的磁盘，系统数据都存储在这个磁盘中。\n  第二个名称是/dev/sdb，容量是 10G，没有任何分区。这就是我们即将交给 OSD 管理的磁盘。\n  使用下面的命令来创建 OSD 进程：\nceph orch daemon add osd *\u0026lt;hostname\u0026gt;*:*\u0026lt;device-name\u0026gt;* 比如，你要在主机ceph-01的名称为/dev/sdb的磁盘上创建 OSD 进程，那么命令应该是：\nceph orch daemon add osd ceph-01:/dev/sdb 这条命令默认不会有输出创建 OSD 进程的详细信息，也就是说，如果该命令很耗时的话，那么你将在什么输出都没有的情况下等待较长时间，这可能令人发慌。你可以加上--verbose参数（缩写为-v），来让它输出详细信息。\n比如：\nceph orch daemon add osd ceph-01:/dev/sdb --verbose   执行完成后，可以使用ceph -s查看当前的集群状态，可以发现已经有一个 OSD 进程加入了进来。\n 使用同样的方法，将所有节点的附加硬盘都加入进来。至此，我们再使用ceph -s查看当前集群的状态，应为HEALTH_OK。\nCeph Filesystem (选做) #   参考资料: CEPH FILE SYSTEM\nCeph 文件系统 (Ceph FS)是个 POSIX 兼容的文件系统，它使用 Ceph 存储集群来存储数据。 Ceph 文件系统与 Ceph 块设备、同时提供 S3 和 Swift API 的 Ceph 对象存储、或者原生库 (librados) 一样，都使用着相同的 Ceph 存储集群系统。\nCeph 文件系统要求 Ceph 存储集群内至少有一个 Ceph 元数据服务器 MDS。\n部署 CephFS #  以上其实都是在搭建 Ceph 集群的环境，我们添加了 3 个 OSD 进程组建起了 Ceph Cluster。接下来，我们便可以在此基础上来具体地使用到 Ceph 所提供的分布式存储能力，从 Ceph 文件系统 CephFS 开始~\nAutomatic Setup 创建 CephFS 的前提是需要至少一个的 MDS daemon 元数据服务器守护进程。其实以下的一条命令即可自动地创建好 MDS daemon、Pool 等：\nceph fs volume create \u0026lt;fs_name\u0026gt; [--placement=\u0026#34;\u0026lt;placement spec\u0026gt;\u0026#34;] 其中，fs_name 是 CephFS 的名称，后面的 --placement 为可选参数，可以通过它来指定 daemon container 跑在哪几个 hosts 上。 参考资料\nMore Customized Setup  参考：下列命令的一些具体参数含义\nceph osd pool create cephfs_data 8 8 # 后面的数量可以调，设大了会无法创建，数值和osd的数量有关，需要是2的倍数 # pool \u0026#39;cephfs_data\u0026#39; created ceph osd pool create cephfs_metadata 8 8 # pool \u0026#39;cephfs_metadata\u0026#39; created ceph fs new cephfs cephfs_metadata cephfs_data # new fs with metadata pool 3 and data pool 2 ceph fs ls # name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] ceph orch apply mds cephfs --placement=\u0026#34;3 node1 node2 node3\u0026#34; # 应用部署CephFS  于是，我们便创建成功了 CephFS。\n在上述方法中，我们还经常看到有 --placement的可选参数，我们可以直接去输各 host 的名字，也可以为每个 host 加 label 标签来进行指定：\nceph orch host ls # 查看当前有的hosts # 可有类似输出 HOST ADDR LABELS STATUS host1 10.1.2.3 host2 10.1.2.4 host3 10.1.2.5 但他们都没有 Label 标签，我们可通过 ceph orch host label add host1 mylabel 添加指定的 Label。\n则 [--placement=\u0026quot;\u0026lt;placement spec\u0026gt;\u0026quot;] 可为 --placement=\u0026quot;label:mylabel\u0026quot;\n想折腾的可能会想到如何删除 CephFS，Ceph 中非常“贴心”地防止你误删除，所以删除起来会有一些麻烦。\nceph fs volume rm \u0026lt;fs_name\u0026gt; [--yes-i-really-mean-it] # 要加上这么一长串后缀，但这样其实还是没法删除 ceph config set mon mon_allow_pool_delete true # 还需要通过这条命令修改ceph config配置  通过 rados df 命令可查看刚才创建的资源池 Pool 的相关信息。 ceph fs ls 可列出 CephFS。 ceph fs status 可查看 CephFS 状态，验证当前已有至少一个 MDS 处在 Active 状态。 还可经常性地执行 ceph -s 查看 Ceph 集群的状态，确保一直处在 HEALTH_OK。   挂载 CephFS #   参考资料：MOUNT CEPHFS USING FUSE\nCephFS 在创建后应当能被实际使用，如完成分布式存储文件的任务。在这一步，我们将把 CephFS 挂载到 Client 端，让 Client 能够创建和存储文件。\n我们先要对 Client 端进行一些配置，保证 Client 端能连接到 MON 主机，即 Bootstrap Host。\n第一步：Generate a minimal conf for the client host. The conf file should be placed at /etc/ceph:\n# on client host mkdir /etc/ceph ssh {user}@{mon-host} \u0026#34;sudo ceph config generate-minimal-conf\u0026#34; | sudo tee /etc/ceph/ceph.conf chmod 644 /etc/ceph/ceph.conf # 赋权 如果不能成功，可直接到 MON 主机执行sudo ceph config generate-minimal-conf，将输出的内容粘贴到 /etc/ceph/ceph.conf（下同）。\n第二步：Create the CephX user and get its secret key:\n# on client host ssh {user}@{mon-host} \u0026#34;sudo ceph fs authorize cephfs client.foo / rw\u0026#34; | sudo tee /etc/ceph/ceph.client.foo.keyring chmod 600 /etc/ceph/ceph.client.foo.keyring # 赋权 在上述命令中，cephfs 是先前所创建的 CephFS 的名称，请将其替换。foo 是 CephX 的用户名，也可自己起。\n以上是前置准备，完成后，我们可通过 ceph-fuse 工具实现文件挂接。如机器上没有，则需要连网安装一下。\n安装完成后，我们可以创建一个被挂接的目录，如 mycephfs：mkdir /mnt/mycephfs\n执行 ceph-fuse -id foo /mnt/mycephfs 即可完成挂接。\n如上条命令无法成功运行，可从 参考资料：MOUNT CEPHFS USING FUSE试一下其他的命令，如 ceph-fuse --id foo -m 192.168.0.1:6789 /mnt/mycephfs，进一步指定了 MON 进程的 IP 和端口号。\n若想取消挂接非常简单，只需 umount /mnt/mycephfs。\n如何判断挂接成功？\n上述命令不报错是一方面，我们也可以通过一些命令来看挂接的情况。\n lsblk 列出所有可用块设备的信息，还能显示他们之间的依赖关系 df -h 查看磁盘占用的空间  通过这些命令，应能看到挂接盘 mycephfs 的存在，查看到其容量等信息。\n Ceph RGW 对象存储 #  Ceph RGW(即 RADOS Gateway)是 Ceph 对象存储网关服务，是基于 LIBRADOS 接口封装实现的 FastCGI 服务，对外提供存储和管理对象数据的 Restful API。对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前 Ceph RGW 兼容常见的对象存储 API，例如兼容绝大部分 Amazon S3 API，兼容 OpenStack Swift API。\n通俗理解是 RGW 作为一个协议转换层，把从上层应用符合 S3 或 Swift 协议的请求转换成 rados 的请求，将数据保存在 rados 集群中。\n 内部概念\n zone：包含多个 RGW 实例的一个逻辑概念。zone 不能跨集群，同一个 zone 的数据保存在同一组 pool 中。 zonegroup：一个 zonegroup 如果包含 1 个或多个 zone。如果一个 zonegroup 包含多个 zone，必须指定一个 zone 作为 master zone，用来处理 bucket 和用户的创建。一个集群可以创建多个 zonegroup，一个 zonegroup 也可以跨多个集群。 realm：一个 realm 包含 1 个或多个 zonegroup。如果 realm 包含多个 zonegroup，必须指定一个 zonegroup 为 master zonegroup， 用来处理系统操作。一个系统中可以包含多个 realm，多个 realm 之间资源完全隔离。  外部概念\n user：对象存储的使用者，默认情况下，一个用户只能创建 1000 个存储桶。 bucket：存储桶，用来管理对象的容器。 object：对象，泛指一个文档、图片或视频文件等，尽管用户可以直接上传一个目录，但是 ceph 并不按目录层级结构保存对象， ceph 所有的对象扁平化的保存在 bucket 中。   参考阅读\n Deploy RGW（选做） #   参考：RGW SERVICE\n To deploy a set of radosgw daemons, with an arbitrary service name name, run the following command:  ceph orch apply rgw *\u0026lt;name\u0026gt;* [--realm=*\u0026lt;realm-name\u0026gt;*] [--zone=*\u0026lt;zone-name\u0026gt;*] --placement=\u0026#34;*\u0026lt;num-daemons\u0026gt;* [*\u0026lt;host1\u0026gt;* ...]\u0026#34; 其中，[]中的内容为可选项，可以都添上。如 ceph orch apply rgw *\u0026lt;name\u0026gt;* --realm=default --zone=default --placement=3。--placement参数的使用和先前实验也是类似的，还可以用 3 node1 node2 node3 来完成指定，以及可以通过 Label 来指定。\n其实这一条命令就够了，然后我们可查看各个 rgw 节点是否已启动：ceph orch ps --daemon-type rgw。\n应能看到 rgw* 均为 running 的 STATUS，则表明顺利启动。若为 starting 可稍等其转为 running。\n如一直显示 unknown/error 的 STATUS，也是因为 pg 资源不足（受 osd 数量所限）导致的，可以将前面创建的 CephFS 删掉，释放资源。\n删除刚刚创建的 unknown/error 的 rgw 的命令为ceph orch rm rgw.*\u0026lt;rgw_name\u0026gt;*\n 在执行上述命令的 Bootstrap host，curl \u0026lt;bootstrap_host_ip:80\u0026gt; 应能看到包含了 \u0026lt;Buckets/\u0026gt; 的 XML 形式的输出。\n使用对象存储（选做） #  我们为 rgw 创建用户：radosgw-admin user create --uid=\u0026lt;username\u0026gt; --display-name=\u0026lt;your_display_name\u0026gt; --system。如 radosgw-admin user create --uid=s3 --display-name=\u0026quot;objcet_storage\u0026quot; --system。执行后，能看到类似输出：\n{ \u0026#34;user_id\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;objcet_storage\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;suspended\u0026#34;: 0, \u0026#34;max_buckets\u0026#34;: 1000, \u0026#34;subusers\u0026#34;: [], \u0026#34;keys\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;x\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;xxxxxxxxkey01\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;xxxxxxxxkey01\u0026#34; } ], \u0026#34;swift_keys\u0026#34;: [], \u0026#34;caps\u0026#34;: [], \u0026#34;op_mask\u0026#34;: \u0026#34;read, write, delete\u0026#34;, \u0026#34;system\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;default_placement\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default_storage_class\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;placement_tags\u0026#34;: [], \u0026#34;bucket_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;user_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;temp_url_keys\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;rgw\u0026#34;, \u0026#34;mfa_ids\u0026#34;: [] } 从命令行的输出中，可以看到 access_key: xxxxxxxxkey01 和 secret_key: xxxxxxxxkey01，我们将其保存下来，后面还要用。\n使用 Ceph 的 RGW 对象存储，可以有很多工具，如 s3cmd、 minio-client 等，这里我们以 s3cmd 为例。有兴趣的还可以尝试 minio可以可视化进行操作。\n如是在 Ubuntu 中，执行 apt install s3cmd 先安装 AWS s3 API，再继续接下来的操作：\n我们执行 s3cmd --configure 去进行相关的配置。\n\u0026gt; Access Key: \u0026#34;xxxxxxxxkey01\u0026#34; \u0026gt; Secret Key: \u0026#34;xxxxxxxxkey02\u0026#34; \u0026gt; S3 Endpoint [s3.amazonaws.com]: \u0026#34;\u0026lt;bootstrap_host_ip:80\u0026gt;\u0026#34;，如 \u0026#34;10.1.1.2:80\u0026#34; DNS-style bucket....[%(bucket)s.s3.amazonaws.com]: \u0026#34;\u0026lt;bootstrap_host_ip:80\u0026gt;/%(bucket)s\u0026#34;，如\u0026#34;10.1.1.2:80/%(bucket)s\u0026#34; HTTPS 选 no，其余基本默认enter 如果 fail 了，可进入到刚刚保存新建的 config：/root/.s3cfg中，参考如下，修改处末尾用了 # 标识：\n[default] access_key = xxxxxxxxkey01 # ... cloudfront_host = 10.1.1.2:80 # ... host_base = 10.1.1.2:80 # host_bucket = 10.1.1.2:80/%(bucket)s # ... secret_key = xxxxxxxxkey02 # ... 查看 Bucket：\ns3cmd ls 一开始没有创建过 Bucket，故没有输出，我们来新建一个 s3cmd mb s3://s3cmd-demo，再执行 s3cmd ls，即可看到新创建的 bucket。\n 参考资料：使用 s3cmd\n参考以上资料，可以尝试继续上传文件、上传文件夹、下载、ls 、删除等命令，体验 ceph-rgw 的 Bucket 与 S3 存储的交互。\n在 Dashboard 里有丰富的信息，可以多多尝试。如查看 rgw 的用户、查看 Bucket、Pool 等，欢迎多多体验。\n如上传文件不成功，有提示：ERROR: S3 error: 416 (InvalidRange) 的错误，同样也可释放掉一些先前创建的资源，如 drop 掉 CephFS 再试试，Dashboard 里也能删除 Pool。OSD 只有 3 个，导致 PG 数量吃紧可能不够用。\n Ceph RBD（选做） #   参考：CEPH BLOCK DEVICE\nRBD 即 RADOS Block Device 的简称，RBD 块存储是最稳定且最常用的存储类型。RBD 块设备类似磁盘可以被挂载。RBD 块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在 Ceph 集群的多个 OSD 中。如下是对 Ceph RBD 的理解：\n RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用； resizable：这个块可大可小； data striped：这个块在 Ceph 里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下； thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U 盘，存了一个 2G 的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间；  块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。\nCeph 可以通过内核模块和 librbd 库提供块设备支持。客户端可以通过内核模块挂在 rbd 使用，客户端使用 rbd 块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过 librbd 使用 ceph 块，典型的是云平台的块存储服务（如下图），云平台可以使用 rbd 作为云的存储后端提供镜像存储、volume 块或者客户的系统引导盘等。\n  创建 RBD  ceph osd pool create rbd 8 # 值调小些，因为云平台资源有限，3个OSD的PG数默认是有上限的  application enable RBD  ceph osd pool application enable rbd rbd  创建 rbd 存储, 指定大小为 1GB  rbd create rbd1 --size 1024  查看 rbd 信息  \u0026gt; rbd --image rbd1 info rbd image \u0026#39;rbd1\u0026#39;: size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: ace58352cf47 block_name_prefix: rbd_data.ace58352cf47 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Wed Dec 1 03:55:11 2021 access_timestamp: Wed Dec 1 03:55:11 2021 modify_timestamp: Wed Dec 1 03:55:11 2021 继续执行以下命令：\n\u0026gt; root@ceph:/mnt# ceph osd crush tunables hammer adjusted tunables profile to hammer \u0026gt; root@ceph:/mnt# ceph osd crush reweight-all reweighted crush hierarchy # 关闭一些内核默认不支持的特性 \u0026gt; root@ceph:/mnt# rbd feature disable rbd1 exclusive-lock object-map fast-diff deep-flatten # 查看特性是否已禁用 \u0026gt; root@ceph:/mnt# rbd --image rbd1 info | grep features features: layering op_features: # 映射到客户端(在需要挂载的客户端运行) \u0026gt; root@ceph:/mnt# rbd map --image rbd1 /dev/rbd0 # 查看映射情况 \u0026gt; root@ceph:/mnt# rbd showmapped id pool namespace image snap device 0 rbd rbd1 - /dev/rbd0 再继续！\n# 格式化磁盘 \u0026gt; root@ceph:/mnt# mkfs.xfs /dev/rbd0 meta-data=/dev/rbd0 isize=512 agcount=8, agsize=32768 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 # 创建挂载目录, 并将 rbd 挂载到指定目录 \u0026gt; root@ceph:/mnt# mkdir /mnt/rbd \u0026gt; root@ceph:/mnt# mount /dev/rbd0 /mnt/rbd/ # 查看挂载情况 \u0026gt; root@ceph:/mnt# df -hl | grep rbd /dev/rbd0 1014M 40M 975M 4% /mnt/rbd 和 CephFS 类似，我们同样可在挂载的目录中创建修改文件，感受 Ceph 的能力——如分布式存储的容错啊，存储共享等。  实验报告模板 #  # Lab03 Ceph 存储实践 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名： \u0026gt; 合作者们：[\u0026#39;学号-姓名\u0026#39;, \u0026#39;学号-姓名\u0026#39;]  --- ## 基本概念思考  回答下列问题： ### Pool 与 Placement Group (PG) 与 Placement Group for Placement purpose (PGP)  ### Ceph 集群的 HEALTH STATUS  ## Ceph 部署  \u0026lt;!-- 本部分内容可以根据部署方式的不同进行不同的改动，这里是使用Cephadm部署的模板 --\u0026gt; ### 实验前置准备  \u0026lt;!-- 可写内容包括： --\u0026gt; \u0026lt;!-- 云平台/本机，操作系统的具体版本，各台机器的Hostname --\u0026gt; \u0026lt;!-- 一些前置工具的安装、配静态IP、改Hostname的记录 --\u0026gt; ### Bootstrap  ### 添加 Host  ### 创建 OSD  ## Ceph Filesystem（选）  ### 部署 CephFS（选）  ### 挂载 CephFS（选）  ## Ceph RGW 对象存储  ### Deploy RGW（选）  ### 使用对象存储（选）  ## Ceph RBD（选）  ## 自行扩展和设计内容（选）  \u0026lt;!-- 本次实验可自行自由进行扩展，体验和尝试Ceph的各项功能，做得*非常非常非常突出*可直接将其作为申优答辩的内容 --\u0026gt; ## 实验总结与心得 "},{"id":4,"href":"/ns-labs/docs/table-of-contents/faq/","title":"FAQ","section":"Table of Contents","content":"FAQ #  如何登录实验虚拟机 #  在云平台中查看虚拟机访问IP、账户以及密码\nWindows win+R 打开“运行”，然后输入mstsc，然后在弹出窗口中输入IP、账号、密码即可 MacOS（Linux？）   Parallels Client Microsoft Remote Desktop （有Bug——或者说是Feature——无法使用macOS本地工具截图） VMware Fusion   云平台上没有显示虚拟机IP或显示的IP无法登录怎么办 #  可以用来登录的IP位于10.251.252.1~10.251.255.254之间\n  如果未登录过实验虚拟机\n 云平台上显示的IP是169或者无IP则暂时无法登录，需要重启虚拟机重新获取IP。需要注意的是，虚拟机的重启、以及DHCP IP的获取都需要时间，开机后应等待3分钟以上，请勿频繁开关机    如果之前登录过实验虚拟机\n 云平台上显示的是192开头的IP或者无IP，可以尝试使用之前的可用IP进行登录。如果无法登录（包括密码错误），请联系助教解决。    重要提醒 #  由于分配的IP均为DHCP自动获取，有可能实验虚拟机的IP会发生变化，尤其是在长时间关机再开启后。因此请登录实验虚拟机后，及时更改虚拟机密码，以避免以下情况发生：\n 未意识到登录了他人的虚拟机， 帮助别人完成了实验工作 自己的虚拟机别别人登录，已经做完的实验被破坏  同时，虚拟机密码请设置非常用敏感密码。因为助教在帮助大家解决问题时往往需要你的密码登录到你的虚拟机。\n用到的安装包、镜像文件如何获取 #    Workstation安装包(Mware-workstation-full-14.1.3-9474260.exe)、ESXi镜像(VMware-VMvisor-Installer-6.5.0-4564106.x86_64-Dell_Customized-A00.iso)、TinyOS镜像(Core-10.1.iso)以及可选使用的CentOS镜像(CentOS-7-x86_64-Minimal-1908.iso)均放在了实验虚拟机桌面上，可以直接使用。\n  vSphere Client安装包可以在 课程中心下载（推荐此方式）或在 临时的资源存放网站下载。\n  同时，所需的资源文件都在 临时的资源存放网站存有备份\n  Workstation与ESXi在本实验中的用处 #  Workstation是用来安装ESXi的。安装ESXi成功后，只要保证Workstation正常运转即可，后续的实验都是通过vSphere (Web) Client在ESXi中完成的\n安装workstation时报错的解决方法（关于 .net framework的错误） #   打开浏览器登录校园网验证，连接网络之后会自动安装补丁修复此错误。\n如何在workstation中载入ESXi镜像 #   在workstation中，创建虚拟机时   载入workstation后界面无反应/鼠标消失怎么办 #  workstation中的虚拟机是会占用鼠标的，按ctrl+alt即可将鼠标焦点从workstation虚拟机中移出。如果安装程序超过2分钟还没有自动启动，重启workstation中的ESXi虚拟机\n安装ESXi时的IP如何配置 #  ESXi的IP不需要特殊配置，使用DHCP分配的动态IP即可。请注意，此IP只在虚拟机中可以有有效访问\n如何登录vSphere Client #  安装 vSpehre Client成功后，打开vSphere Client，输入ESXi显示的IP，以及安装ESXi过程中设置的账号密码即可登录。\n使用web client还是传统的本地client #  本次实验大部分操作要求在本地client中完成。如果遇到本地client无法完成的操作，则要登录web client完成\n访问web client时报错的解决方法 #   按ESC键忽略此错误即可\n创建用户时提示“用户名或密码格式无效”的解决方法 #  密码要求包含数字、大小写字母以及符号，缺一不可\n关于进行虚拟机设置时，使用vSphere Client提示“无法使用vSphere Client编辑版本为12或更高版本的虚拟机的设置”的解决方法 #  1、创建虚拟机时，第一个页面“配置”不选择“典型”，选择“自定义”，并在“虚拟机版本”处选择虚拟机版本11或以下版本。 2、使用Web客户端登录ESXi管理页面，在虚拟机浏览器中输入ESXi地址即可访问。 以上两种方法使用一种即可。\n开启虚拟机电源后，打开本地vSphere Client的虚拟机控制台显示黑屏 #  换用Web客户端访问。\n安装虚拟机操作系统时显示找不到OS #   确保连接了正确的镜像文件 确保虚拟光驱的状态为“已连接”（可在设置中更改）  一般可以自动检测，无需进入BIOS调整启动顺序或手动选择启动选项，但如果排查过上面的情况仍然无法安装操作系统，可以手动进入BIOS更改一下引导选项。\n虚拟机无法连接外网 #   确保设置了正确的DNS、已经通过gw认证登录 如果无法访问认证页面，可以在控制面板\\网络和 Internet\\网络连接中查看是否有多余的网卡（不包括VMware Network Adapter），查看这些网卡的IP，将多余的网卡禁用（注意不要禁用错了，要保留IP地址与远程登录地址相同的网卡）  "},{"id":5,"href":"/ns-labs/docs/table-of-contents/","title":"Table of Contents","section":"Docs","content":"你好，网存！ #  欢迎来到网络存储的实验课堂！\n目录 #  实验目录可见左侧边栏\n"},{"id":6,"href":"/ns-labs/docs/resources/","title":"Resources","section":"Docs","content":"课程相关资源 #  此处存放了课程相关资源，如系统 ISO 镜像和各种软件的下载地址。\n"}]